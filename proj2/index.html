<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Diffusion Models – Visualizations</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Manrope:wght@300;400;600;700&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="../styles.css" />
</head>

<body class="top-title">

  <canvas id="sky" aria-hidden="true"></canvas>
  <header class="site-header">
    <div class="content">
      <nav class="top-nav" aria-label="Project navigation">
        <a href="../../index.html" class="nav-logo">
          Home
        </a>
      </nav>
    </div>
  </header>

  <div class="content big-title">
    <h1 class="title">Filters and Frequencies</h1>
  </div>

  <!-- ============================================ 1.1 Convolutions from Scratch ===================================== -->
  <section class="content gold-glass iteration-belt" id="part1.1">
    <h2 class="section-heading">1.1 Convolutions from Scratch</h2>

    <div class="iteration-belt__intro">
      <p>
        A discrete convolution can be viewed as sliding a
        small kernel over the image and, at each location, computing a weighted sum of the local
        neighborhood with zero-padding so that the output has the same spatial size as the input.
      </p>
      <p>
        Three implementations are compared: a naïve four-loop baseline, a partially vectorized
        two-loop NumPy version, and SciPy’s optimized <code>convolve2d</code>. The resulting
        blurred images are visually indistinguishable, which confirms that all three
        implementations realize the same filter while differing only in efficiency and internal
        optimization.
      </p>
    </div>

    <div class="iteration-belt__header">
      <h3 class="iteration-belt__title">9×9 box filter on self-portrait</h3>
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">Pause</button>
    </div>

    <div class="iteration-belt__viewport">
      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/1_convolutions/conv_four_loops.png"
            alt="Self-portrait blurred with four-loop convolution">
          <figcaption class="iteration-belt__caption">
            Four-loop convolution (direct discrete implementation)
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/1_convolutions/conv_two_loops.png"
            alt="Self-portrait blurred with two-loop convolution">
          <figcaption class="iteration-belt__caption">
            Two-loop convolution (window extraction + vectorized sum)
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/1_convolutions/conv_scipy.png"
            alt="Self-portrait blurred with SciPy convolve2d">
          <figcaption class="iteration-belt__caption">
            SciPy <code>convolve2d</code> reference implementation
          </figcaption>
        </figure>
      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        A benchmark on the 9×9 box filter, averaged over five runs,
        quantifies these differences. The naïve four-loop implementation takes about
        9066.8&nbsp;ms per image, the two-loop implementation 1634.2&nbsp;ms, and the SciPy
        reference 70.5&nbsp;ms. Moving from four loops to two loops yields a speedup of roughly
        5.5×, while switching from four loops to SciPy gives a speedup of about 129×; SciPy is
        still roughly 23× faster than the partially vectorized version. These measurements show
        that writing convolution in terms of large array operations rather than explicit Python
        loops is essential for scaling image filtering to realistic workloads.
      </p>
    </div>

    <div class="iteration-belt__outro">
      <p>
        All three filtered selfies smooth high-frequency detail in fine details like hair, pavement, and jacket
        while preserving coarse structure such as the outline of my face and bike rack. It is therefore true
        that the custom NumPy convolutions are numerically consistent with the library
        routine and differ primarily in runtime, with increasing vectorization yielding faster
        execution for the same visual effect.
      </p>
    </div>


  </section>

  <div class="code-snippets">
    <h3 class="code-snippets__title">Convolution implementations (Python)</h3>

    <!-- 1. pad_zero -->
    <details class="code-snippet">
      <summary>pad_zero</summary>
      <div class="code-snippet__body">
        <pre class="code-block"><code class="language-python">import numpy as np

def pad_zero(img: np.ndarray, pad_h: int, pad_w: int) -&gt; np.ndarray:
    """Zero-pad a 2D image by (pad_h, pad_w) on all sides."""
    assert img.ndim == 2
    H, W = img.shape
    out = np.zeros((H + 2 * pad_h, W + 2 * pad_w), dtype=img.dtype)
    out[pad_h: pad_h + H, pad_w: pad_w + W] = img
    return out</code></pre>
      </div>
    </details>

    <!-- 2. conv2d_four_loops -->
    <details class="code-snippet">
      <summary>conv2d_four_loops</summary>
      <div class="code-snippet__body">
        <pre class="code-block"><code class="language-python">def conv2d_four_loops(img: np.ndarray, kernel: np.ndarray) -&gt; np.ndarray:
    """
    Naive 2D convolution using 4 nested loops and zero padding.

    img:    2D grayscale image
    kernel: 2D filter kernel
    """
    assert img.ndim == 2
    # flip kernel for convolution
    k = np.flipud(np.fliplr(kernel))
    kh, kw = k.shape
    ph, pw = kh // 2, kw // 2

    padded = pad_zero(img, ph, pw)
    H, W = img.shape
    out = np.zeros_like(img, dtype=np.float32)

    for y in range(H):
        for x in range(W):
            acc = 0.0
            for j in range(kh):
                for i in range(kw):
                    acc += padded[y + j, x + i] * k[j, i]
            out[y, x] = acc
    return out</code></pre>
      </div>
    </details>

    <!-- 3. conv2d_two_loops -->
    <details class="code-snippet">
      <summary>conv2d_two_loops</summary>
      <div class="code-snippet__body">
        <pre class="code-block"><code class="language-python">def conv2d_two_loops(img: np.ndarray, kernel: np.ndarray) -&gt; np.ndarray:
    """
    More efficient convolution: 2 loops over pixels, inner sum done by NumPy.
    """
    assert img.ndim == 2
    k = np.flipud(np.fliplr(kernel))
    kh, kw = k.shape
    ph, pw = kh // 2, kw // 2

    padded = pad_zero(img, ph, pw)
    H, W = img.shape
    out = np.zeros_like(img, dtype=np.float32)

    for y in range(H):
        # slice the relevant "row band" once per y
        row = padded[y: y + kh, :]
        for x in range(W):
            patch = row[:, x: x + kw]
            out[y, x] = float(np.sum(patch * k))
    return out</code></pre>
      </div>
    </details>
  </div>

  <!-- ============================================ 1.2 Finite Difference Operator ==================================== -->
  <section class="content gold-glass iteration-belt" id="part1-2">
    <h2 class="section-heading">1.2 Finite Difference Operator</h2>

    <div class="iteration-belt__intro">
      <p>
        We also study how simple finite difference filters approximate image gradients on the
        standard cameraman photograph. A horizontal difference kernel measures changes in intensity
        from left to right, while a vertical difference kernel measures changes from top to bottom.
        Applying these kernels yields directional derivatives that respond strongly where the image
        contains vertical or horizontal edges and remain near zero in smooth, slowly varying regions.
      </p>
      <p>
        The directional responses are combined into a gradient magnitude image that summarizes edge
        strength independently of direction, and a global threshold is used to convert this
        continuous signal into a binary edge map. Furthermore, two color visualizations of gradient
        orientation are produced: one using a fast approximation and one using a more expensive
        arctangent computation. Comparing these views reveals both the structure of the cameraman
        scene and the extent to which the approximate orientation method matches the reference.
      </p>
    </div>

    <div class="iteration-belt__header">
      <h3 class="iteration-belt__title">Cameraman gradients, magnitudes, and edges</h3>
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">Pause</button>
    </div>

    <div class="iteration-belt__viewport">
      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/2_finitie_difference_operator/cameraman.png"
            alt="Original cameraman image">
          <figcaption class="iteration-belt__caption">
            Original cameraman image (grayscale reference for all derivative visualizations).
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/2_finitie_difference_operator/Ix.png"
            alt="Horizontal derivative of the cameraman image">
          <figcaption class="iteration-belt__caption">
            Horizontal derivative: vertical structures such as the tripod legs and building edges
            appear as bright responses where intensity changes strongly left-to-right.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/2_finitie_difference_operator/Iy.png"
            alt="Vertical derivative of the cameraman image">
          <figcaption class="iteration-belt__caption">
            Vertical derivative: horizontal structures such as the horizon and rooftop lines
            dominate the response, while sky and grass remain near zero.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/2_finitie_difference_operator/grad_mag.png"
            alt="Gradient magnitude of the cameraman image">
          <figcaption class="iteration-belt__caption">
            Gradient magnitude: combines horizontal and vertical changes into a single measure of
            edge strength, concentrating high values along object boundaries.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/2_finitie_difference_operator/edges.png"
            alt="Binary edge map from thresholded gradient magnitude">
          <figcaption class="iteration-belt__caption">
            Binary edge map: thresholded gradient magnitude retains the main contours of the
            cameraman, tripod, and skyline while suppressing small noisy responses.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/2_finitie_difference_operator/orientation_fast.png"
            alt="Gradient orientation visualization using fast approximation">
          <figcaption class="iteration-belt__caption">
            Orientation visualization (fast approximation): edge direction is encoded as hue, with
            saturation driven by gradient strength, revealing how directions vary smoothly along
            curved contours.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/2_finitie_difference_operator/orientation_atan.png"
            alt="Gradient orientation visualization using arctangent reference">
          <figcaption class="iteration-belt__caption">
            Orientation visualization (arctangent reference): produces almost identical color
            patterns, confirming that the approximate method recovers the same underlying
            orientation field at lower computational cost.
          </figcaption>
        </figure>
      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        Taken together, these visualizations explain how finite difference operators extract an abudance of
        geometric information from a single grayscale image. The directional derivatives separate
        vertical and horizontal content, while gradient magnitude emphasizes where edges are strongest
        Thresholded map allow isolation of clean structural contours, and the orientation fields add
        directional information which are crucial for tasks pretaining to corner detection and feature
        description. The close match between the fast and reference orientation images demonstrates
        that the approximate computation is accurate enough for downstream use while being more
        efficient.
      </p>
    </div>
  </section>


  <!-- =============================== 1.3 Derivative of Gaussian Filter ================================= -->
  <section class="content gold-glass iteration-belt" id="part1-3">
    <h2 class="section-heading">1.3 Derivative of Gaussian Filter</h2>

    <div class="iteration-belt__intro">
      <p>
        This section compares two strategies for computing smoothed image gradients: first blurring
        the image with a Gaussian and then applying finite difference filters, versus convolving
        once with derivative-of-Gaussian kernels. Because convolution is linear and shift-invariant,
        differentiating the Gaussian and convolving the result with the image should be equivalent
        to convolving with the Gaussian and differentiating afterward, up to numerical details and
        boundary effects.
      </p>
      <p>
        The experiment constructs horizontal and vertical derivative-of-Gaussian kernels and applies
        them to the cameraman image. The resulting directional derivatives and gradient magnitudes
        are compared against those obtained from the “smooth then difference” pipeline. The
        comparison tests whether the one-step derivative-of-Gaussian approach reproduces the same
        cleaned-up edges and suppressed noise as the two-stage method that explicitly separates
        smoothing and differentiation.
      </p>
    </div>

    <div class="iteration-belt__header">
      <h3 class="iteration-belt__title">Gaussian smoothing vs. derivative-of-Gaussian</h3>
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">Pause</button>
    </div>

    <div class="iteration-belt__viewport">
      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/DoGx_kernel.png"
            alt="Horizontal derivative-of-Gaussian kernel">
          <figcaption class="iteration-belt__caption">
            Horizontal DoG kernel: a smoothed, antisymmetric pattern that responds to vertical
            edges while attenuating high-frequency noise.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/DoGy_kernel.png"
            alt="Vertical derivative-of-Gaussian kernel">
          <figcaption class="iteration-belt__caption">
            Vertical DoG kernel: rotated counterpart used to detect horizontal edges with the same
            built-in smoothing.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/Ix_smooth_then_diff.png"
            alt="Horizontal derivative after Gaussian smoothing then finite difference">
          <figcaption class="iteration-belt__caption">
            Horizontal gradient from “smooth then difference”: vertical contours of the tripod,
            coat, and buildings remain sharp, but background noise is strongly reduced.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/Iy_smooth_then_diff.png"
            alt="Vertical derivative after Gaussian smoothing then finite difference">
          <figcaption class="iteration-belt__caption">
            Vertical gradient from “smooth then difference”: horizontal structures such as the
            horizon and rooftops dominate the response with fewer speckles in the sky and grass.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/grad_smooth_then_diff.png"
            alt="Gradient magnitude after Gaussian smoothing then finite difference">
          <figcaption class="iteration-belt__caption">
            Gradient magnitude for “smooth then difference”: a clean edge map where strong
            contours are preserved and fine-grained noise has been suppressed by the initial blur.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/Ix_DoG.png"
            alt="Horizontal derivative from derivative-of-Gaussian convolution">
          <figcaption class="iteration-belt__caption">
            Horizontal gradient from DoG convolution: closely matches the smooth-then-differentiate
            result, confirming that the DoG kernel embeds both smoothing and differentiation.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/Iy_DoG.png"
            alt="Vertical derivative from derivative-of-Gaussian convolution">
          <figcaption class="iteration-belt__caption">
            Vertical gradient from DoG convolution: edge strengths and locations align with the
            corresponding smooth-then-difference vertical derivative.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/grad_DoG.png"
            alt="Gradient magnitude from derivative-of-Gaussian convolution">
          <figcaption class="iteration-belt__caption">
            Gradient magnitude from DoG convolution: nearly indistinguishable from the
            smooth-then-difference magnitude, demonstrating the equivalence of the two pipelines in
            both structure and edge strength.
          </figcaption>
        </figure>
      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        The side-by-side comparison shows that Gaussian smoothing followed by finite differencing
        and direct convolution with derivative-of-Gaussian kernels produce almost identical gradient
        fields and edge magnitudes. Any remaining discrepancies are subtle and attributable to
        numerical precision or padding choices rather than to the operators themselves. This
        validates derivative-of-Gaussian filters as a compact, one-step implementation of the
        “blur then differentiate” pipeline, making them a convenient and robust building block for
        edge detection and feature extraction.
      </p>
    </div>
  </section>


  <!-- ============================================ 2.1 Image Sharpening (Unsharp Masking) =========================== -->
  <section class="content gold-glass iteration-belt" id="part2-1">
    <h2 class="section-heading">2.1 Image Sharpening with Unsharp Masking</h2>

    <div class="iteration-belt__intro">
      <p>
        This section uses unsharp masking to enhance fine detail in a photograph of the Taj Mahal.
        The procedure first forms a smoothed version of the image by convolving with a Gaussian
        blur kernel, then subtracts this low-pass version from the original to isolate a high-pass
        component that contains edges and textures. A scaled copy of this high-pass signal is added
        back to the original image, producing a sharpened result in which local contrast near
        boundaries is increased while global brightness and color are largely preserved.
      </p>
      <p>
        Interpreted in terms of kernels, unsharp masking combines an impulse-like identity kernel
        with a negative Gaussian to create an effective filter that has a strong positive center and
        a weaker negative surround. This structure boosts rapid spatial changes and slightly
        suppresses slowly varying regions. The deliverables show each stage of this pipeline—the
        blur, the extracted high-pass component, the sharpened output, and the equivalent single
        unsharp kernel—to make clear how the operation redistributes contrast across spatial scales.
      </p>
    </div>

    <div class="iteration-belt__header">
      <h3 class="iteration-belt__title">Unsharp masking pipeline on the Taj Mahal</h3>
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">Pause</button>
    </div>

    <div class="iteration-belt__viewport">
      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/unsharp_kernel.png"
            alt="Gaussian blur kernel used for unsharp masking">
          <figcaption class="iteration-belt__caption">
            Gaussian blur kernel: a small, isotropic low-pass filter that produces a softened version
            of the image and suppresses high-frequency noise.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/blurred.png" alt="Blurred Taj Mahal image">
          <figcaption class="iteration-belt__caption">
            Blurred image: large-scale structure of the Taj Mahal is preserved, but fine architectural
            details and textures in the trees and walkway are noticeably washed out.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/highpass.png"
            alt="High-pass component obtained by subtracting blur from the original">
          <figcaption class="iteration-belt__caption">
            High-pass component: subtraction of the blurred image from the original isolates edges and
            fine detail. The building outline, ornamental patterns, and strong intensity transitions
            appear as bright responses on a dark background.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/sharpened.png"
            alt="Sharpened Taj Mahal image after unsharp masking">
          <figcaption class="iteration-belt__caption">
            Sharpened result: adding a scaled high-pass signal back to the original yields a crisper
            image. The dome ridges, facade carvings, and tree silhouettes gain local contrast without
            creating strong halos or amplifying background noise.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/sharpened_kernel.png"
            alt="Effective unsharp masking kernel">
          <figcaption class="iteration-belt__caption">
            Effective unsharp kernel: combining the impulse (identity) with a negative Gaussian blur
            produces a single convolution kernel whose positive center and negative surround implement
            “original plus scaled high-pass” in one step.
          </figcaption>
        </figure>
      </div>
    </div>

    <div class="iteration-belt__intro">
      <p>
        The same unsharp masking pipeline is then applied to a scanned vintage family photograph as
        a second case study. Here the blur uses a broader Gaussian with a larger kernel size and
        standard deviation so that the high-pass component captures mid-scale structures such as
        facial features, clothing folds, and plant outlines while averaging over film grain and
        digitization noise. The deliverables again include the blur, the extracted high-frequency
        content, the sharpened image, and the effective kernel.
      </p>
    </div>

    <div class="iteration-belt__header">
      <h3 class="iteration-belt__title">Unsharp masking on a vintage family scan</h3>
      <!-- <button type="button" class="iteration-belt__toggle" aria-pressed="false">Pause</button> -->
    </div>

    <div class="iteration-belt__viewport">
      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/grandpa/unsharp_kernel.png"
            alt="Gaussian blur kernel used to smooth the vintage photo">
          <figcaption class="iteration-belt__caption">
            Gaussian blur kernel used to define the low-pass component for the vintage photograph,
            tuned to treat facial features and clothing folds as detail while smoothing over finer
            grain.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/grandpa/gp.jpeg"
            alt="Original vintage family photograph before sharpening">
          <figcaption class="iteration-belt__caption">
            Original scan: a slightly faded, low-contrast family photograph with modest blur and
            visible texture from aging and digitization.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/grandpa/blurred.png"
            alt="Blurred version of the vintage family photograph">
          <figcaption class="iteration-belt__caption">
            Blurred image: large shapes such as the figure and background plants remain, but fine
            details in the face, shirt, and foliage are smoothed away by the Gaussian filter.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/grandpa/highpass.png"
            alt="High-pass component of the vintage family photograph">
          <figcaption class="iteration-belt__caption">
            High-pass component: subtraction of the blur from the original isolates edges and local
            contrast changes, outlining the figure, clothing seams, and wire fence while largely
            suppressing uniform regions.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/grandpa/sharpened.png"
            alt="Sharpened vintage family photograph after unsharp masking">
          <figcaption class="iteration-belt__caption">
            Sharpened result: adding the high-pass signal back restores crispness to facial
            features and surrounding vegetation, making the photograph feel clearer without
            introducing strong halos or emphasizing noise artifacts.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/grandpa/sharpened_kernel.png"
            alt="Effective unsharp masking kernel for the vintage family photograph">
          <figcaption class="iteration-belt__caption">
            Effective unsharp kernel: the central impulse and negative Gaussian surround together
            implement the entire “original plus scaled high-pass” operation as a single convolution,
            tuned to the softer focus and lower resolution of the scan.
          </figcaption>
        </figure>
      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        Comparing the Taj Mahal and vintage photograph examples highlights how the unsharp masking
        parameters control the notion of “detail” being enhanced. A narrower blur sharpens fine
        architectural edges in a high-resolution scene, while a broader blur reserved for the scan
        targets mid-scale structure and prevents film grain from becoming visually dominant. In both
        cases, the method operates by selectively boosting high-frequency content relative to a
        smooth background, yielding images that appear clearer and more focused without relying on a
        simple global contrast stretch.
      </p>
    </div>
  </section>

  <!-- ============================================ 2.2 Hybrid Images ================================================ -->
  <section class="content gold-glass iteration-belt" id="part2-2">
    <h2 class="section-heading">2.2 Hybrid Images</h2>

        <div class="iteration-belt__intro">
      <p>
        A hybrid image combines the low frequencies of one photograph with the high frequencies of
        another so that perception changes with viewing distance. In this example, Derek’s face
        supplies the coarse, low-frequency structure, while Nutmeg contributes the fine,
        high-frequency detail such as fur and whiskers. The filtering parameters control how sharply
        these two bands are separated and how strongly the high-frequency content is reintroduced
        into the final composition.
      </p>
      <p>
        The portrait is heavily blurred with a Gaussian filter, producing a smooth low-pass image in
        which small-scale details are suppressed but the overall facial geometry and shading remain
        intact. The cat image is transformed into a high-pass signal by subtracting a softened
        version of itself, yielding an essentially zero-mean edge map dominated by sharp contours
        and texture. Before the two components are added, the high-pass layer is attenuated so that
        cat features are clearly visible at close range but do not overwhelm the underlying
        low-frequency portrait when the hybrid is viewed from farther away.
      </p>
    </div>



    <div class="iteration-belt__header">
      <h3 class="iteration-belt__title">Constructing a hybrid portrait–cat image</h3>
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">Pause</button>
    </div>

    <div class="iteration-belt__viewport">
      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/2_hybrid/demo/A_stage1_aligned_cropped.png"
            alt="Aligned portrait used for low-pass component">
          <figcaption class="iteration-belt__caption">
            Aligned portrait (Derek): source for the low-frequency content of the hybrid. Only the
            global layout of the face and illumination are intended to survive the subsequent blur.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/2_hybrid/demo/B_stage1_aligned_cropped.png"
            alt="Aligned cat image used for high-pass component">
          <figcaption class="iteration-belt__caption">
            Aligned cat (Nutmeg): source for high-frequency detail. Whiskers, fur strands, and sharp
            eye boundaries generate the high-pass structure that dominates close-up perception.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/2_hybrid/demo/lowpass.png" alt="Low-pass filtered portrait">
          <figcaption class="iteration-belt__caption">
            Low-pass portrait with <code>ksize = 53, sigma = 8.0</code>: the wide Gaussian blur
            produces a very smooth image where fine texture is almost completely removed but the
            coarse arrangement of facial features remains clearly visible.
          </figcaption>
        </figure>


        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/2_hybrid/demo/lowpass_fft.png"
            alt="Log-magnitude Fourier spectrum of low-pass portrait">
          <figcaption class="iteration-belt__caption">
            Low-pass Fourier magnitude: a bright blob concentrated at the center with horizontal and
            vertical streaks. This indicates that energy is almost entirely in low frequencies,
            matching the visually smooth low-pass portrait.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/2_hybrid/demo/highpass.png"
            alt="High-pass filtered cat image">
          <figcaption class="iteration-belt__caption">
            High-pass cat with <code>ksize = 27, sigma = 3.1</code>: subtracting the blurred cat
            isolates edges and mid–high-frequency texture, yielding an almost embossed appearance
            with strong responses on whiskers and fur.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/2_hybrid/demo/highpass_fft.png"
            alt="Log-magnitude Fourier spectrum of high-pass cat image">
          <figcaption class="iteration-belt__caption">
            High-pass Fourier magnitude: the central region is darkened, while energy appears in a
            ring and diagonal streaks further from the origin. These oriented bands correspond to
            Nutmeg’s whiskers and fur, confirming that low frequencies have been suppressed while
            edge-dominated components remain.
          </figcaption>
        </figure>



        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/2_hybrid/demo/hybrid.png"
            alt="Hybrid image combining low-pass portrait and high-pass cat">
          <figcaption class="iteration-belt__caption">
            Hybrid image: adding <code>alpha · high</code> to the low-pass portrait yields a face
            that appears feline at close range but transitions to Derek’s portrait when the image is
            viewed from far away or at reduced resolution. The chosen <code>alpha = 0.2</code>
            balances visibility of Nutmeg’s detail with the stability of the large-scale portrait.
          </figcaption>
        </figure>



        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/2_hybrid/demo/hybrid_fft.png"
            alt="Log-magnitude Fourier spectrum of hybrid image">
          <figcaption class="iteration-belt__caption">
            Hybrid Fourier magnitude: combines the bright central blob of the Derek low-pass
            spectrum with the outer high-frequency streaks from the cat. The spectrum literally
            appears as the sum of the previous two, mirroring how spatial low- and high-frequency
            content coexist in the hybrid.
          </figcaption>
        </figure>
      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        The resulting hybrid behaves as expected: close inspection is dominated by Nutmeg’s
        high-frequency texture, while increased viewing distance or downsampling removes these
        frequencies and reveals the underlying Derek portrait. The Fourier visualizations tie this
        perceptual effect directly to the frequency decomposition: the low-pass image occupies only
        a compact region near the origin, the high-pass image populates outer bands with oriented
        energy, and the hybrid spectrum exhibits both components simultaneously. Appropriate
        choices of kernel size, standard deviation, and scaling factor therefore control which
        spatial scales belong to each subject and how strongly each one contributes to the final
        illusion.
      </p>
    </div>
  </section>





  <script src="../script.js"></script>
</body>

</html>