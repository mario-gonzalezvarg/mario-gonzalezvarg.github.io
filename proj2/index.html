<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Diffusion Models – Visualizations</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Manrope:wght@300;400;600;700&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="../styles.css" />
</head>

<body class="top-title">

  <canvas id="sky" aria-hidden="true"></canvas>
  <header class="site-header">
    <div class="content">
      <nav class="top-nav" aria-label="Project navigation">
        <a href="../../index.html" class="nav-logo">
          Home
        </a>
      </nav>
    </div>
  </header>

  <div class="content big-title">
    <h1 class="title">Filters and Frequencies</h1>
  </div>

  <!-- ============================================ 1.1 Convolutions from Scratch ===================================== -->
  <section class="content gold-glass iteration-belt" id="part1.1">
    <h2 class="section-heading">1.1 Convolutions from Scratch</h2>

    <div class="iteration-belt__intro">
      <p>
        Let us demonstarte a 2D convolution implementations by applying a 9×9 box
        filter to a grayscale self-portrait. A discrete convolution can be viewed as sliding a
        small kernel over the image and, at each location, computing a weighted sum of the local
        neighborhood with zero-padding so that the output has the same spatial size as the input.
      </p>
      <p>
        Three implementations are compared: a naïve four-loop baseline, a partially vectorized
        two-loop NumPy version, and SciPy’s optimized <code>convolve2d</code>. The resulting
        blurred portraits are visually indistinguishable, which confirms that all three
        implementations realize the same filter while differing only in efficiency and internal
        optimization.
      </p>
    </div>

    <div class="iteration-belt__header">
      <h3 class="iteration-belt__title">9×9 box filter on self-portrait</h3>
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">Pause</button>
    </div>

    <div class="iteration-belt__viewport">
      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/1_convolutions/conv_four_loops.png"
            alt="Self-portrait blurred with four-loop convolution">
          <figcaption class="iteration-belt__caption">
            Four-loop convolution (direct discrete implementation)
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/1_convolutions/conv_two_loops.png"
            alt="Self-portrait blurred with two-loop convolution">
          <figcaption class="iteration-belt__caption">
            Two-loop convolution (window extraction + vectorized sum)
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/1_convolutions/conv_scipy.png"
            alt="Self-portrait blurred with SciPy convolve2d">
          <figcaption class="iteration-belt__caption">
            SciPy <code>convolve2d</code> reference implementation
          </figcaption>
        </figure>


      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        All three filtered selfies smooth high-frequency detail in the hair, pavement, and jacket
        while preserving coarse structure such as the outline of the face and bike rack. This
        confirms that the custom NumPy convolutions are numerically consistent with the library
        routine and differ primarily in runtime, with increasing vectorization yielding faster
        execution for the same visual effect.
      </p>
    </div>
  </section>

  <div class="code-snippets">
    <h3 class="code-snippets__title">Convolution implementations (Python)</h3>

    <!-- 1. pad_zero -->
    <details class="code-snippet">
      <summary>pad_zero</summary>
      <div class="code-snippet__body">
        <pre class="code-block"><code class="language-python">import numpy as np

def pad_zero(img: np.ndarray, pad_h: int, pad_w: int) -&gt; np.ndarray:
    """Zero-pad a 2D image by (pad_h, pad_w) on all sides."""
    assert img.ndim == 2
    H, W = img.shape
    out = np.zeros((H + 2 * pad_h, W + 2 * pad_w), dtype=img.dtype)
    out[pad_h: pad_h + H, pad_w: pad_w + W] = img
    return out</code></pre>
      </div>
    </details>

    <!-- 2. conv2d_four_loops -->
    <details class="code-snippet">
      <summary>conv2d_four_loops</summary>
      <div class="code-snippet__body">
        <pre class="code-block"><code class="language-python">def conv2d_four_loops(img: np.ndarray, kernel: np.ndarray) -&gt; np.ndarray:
    """
    Naive 2D convolution using 4 nested loops and zero padding.

    img:    2D grayscale image
    kernel: 2D filter kernel
    """
    assert img.ndim == 2
    # flip kernel for convolution
    k = np.flipud(np.fliplr(kernel))
    kh, kw = k.shape
    ph, pw = kh // 2, kw // 2

    padded = pad_zero(img, ph, pw)
    H, W = img.shape
    out = np.zeros_like(img, dtype=np.float32)

    for y in range(H):
        for x in range(W):
            acc = 0.0
            for j in range(kh):
                for i in range(kw):
                    acc += padded[y + j, x + i] * k[j, i]
            out[y, x] = acc
    return out</code></pre>
      </div>
    </details>

    <!-- 3. conv2d_two_loops -->
    <details class="code-snippet">
      <summary>conv2d_two_loops</summary>
      <div class="code-snippet__body">
        <pre class="code-block"><code class="language-python">def conv2d_two_loops(img: np.ndarray, kernel: np.ndarray) -&gt; np.ndarray:
    """
    More efficient convolution: 2 loops over pixels, inner sum done by NumPy.
    """
    assert img.ndim == 2
    k = np.flipud(np.fliplr(kernel))
    kh, kw = k.shape
    ph, pw = kh // 2, kw // 2

    padded = pad_zero(img, ph, pw)
    H, W = img.shape
    out = np.zeros_like(img, dtype=np.float32)

    for y in range(H):
        # slice the relevant "row band" once per y
        row = padded[y: y + kh, :]
        for x in range(W):
            patch = row[:, x: x + kw]
            out[y, x] = float(np.sum(patch * k))
    return out</code></pre>
      </div>
    </details>
  </div>


  <!-- ============================================ 1.2 Finite Difference Operator ==================================== -->
  <section class="content gold-glass iteration-belt" id="part1-2">
    <h2 class="section-heading">1.2 Finite Difference Operator</h2>

    <div class="iteration-belt__intro">
      <p>
        This section uses finite difference filters to approximate the image gradient of the
        standard cameraman image. A horizontal difference kernel measures how intensities change
        left-to-right, while a vertical difference kernel measures changes top-to-bottom. These
        directional responses are then combined into a gradient magnitude and thresholded to
        produce a binary edge map, and the gradient orientation is visualized in color to reveal
        how edge directions vary across the scene.
      </p>
    </div>

    <div class="iteration-belt__header">
      <h3 class="iteration-belt__title">Cameraman gradients, magnitudes, and edges</h3>
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">Pause</button>
    </div>

    <div class="iteration-belt__viewport">
      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/2_finitie_difference_operator/cameraman.png"
            alt="Original cameraman image">
          <figcaption class="iteration-belt__caption">
            Original cameraman image (grayscale reference for all derivative visualizations).
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/2_finitie_difference_operator/Ix.png"
            alt="Horizontal derivative of the cameraman image">
          <figcaption class="iteration-belt__caption">
            Horizontal derivative: vertical structures such as the tripod legs and building edges
            appear as bright responses where intensity changes strongly left-to-right.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/2_finitie_difference_operator/Iy.png"
            alt="Vertical derivative of the cameraman image">
          <figcaption class="iteration-belt__caption">
            Vertical derivative: horizontal structures such as the horizon and rooftop lines
            dominate the response, while sky and grass remain near zero.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/2_finitie_difference_operator/grad_mag.png"
            alt="Gradient magnitude of the cameraman image">
          <figcaption class="iteration-belt__caption">
            Gradient magnitude: combines horizontal and vertical changes into a single measure of
            edge strength, concentrating high values along object boundaries.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/2_finitie_difference_operator/edges.png"
            alt="Binary edge map from thresholded gradient magnitude">
          <figcaption class="iteration-belt__caption">
            Binary edge map: thresholded gradient magnitude retains the main contours of the
            cameraman, tripod, and skyline while suppressing small noisy responses.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/2_finitie_difference_operator/orientation_fast.png"
            alt="Gradient orientation visualization using fast approximation">
          <figcaption class="iteration-belt__caption">
            Orientation visualization (fast approximation): edge direction is encoded as hue, with
            saturation driven by gradient strength, revealing how directions vary smoothly along
            curved contours.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/2_finitie_difference_operator/orientation_atan.png"
            alt="Gradient orientation visualization using arctangent reference">
          <figcaption class="iteration-belt__caption">
            Orientation visualization (arctangent reference): produces almost identical color
            patterns, confirming that the approximate method recovers the same underlying
            orientation field at lower computational cost.
          </figcaption>
        </figure>
      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        Together, these views show how simple finite difference filters extract rich geometric
        structure from a single grayscale image. The partial derivatives differentiate vertical and
        horizontal content, the gradient magnitude highlights where edges are strongest, the
        thresholded map isolates clean structural contours, and the orientation visualizations add
        directional information that is essential for downstream tasks such as corner detection and
        feature description.
      </p>
    </div>
  </section>

  <!-- =============================== 1.3 Derivative of Gaussian Filter ================================= -->
  <section class="content gold-glass iteration-belt" id="part1-3">
    <h2 class="section-heading">1.3 Derivative of Gaussian Filter</h2>

    <div class="iteration-belt__intro">
      <p>
        This section compares two equivalent ways of computing image gradients while suppressing
        noise: (1) smoothing the image with a Gaussian and then applying finite difference
        operators, and (2) convolving once with <em>derivative-of-Gaussian</em> (DoG) kernels.
        Because convolution is linear and shift-invariant, differentiating the Gaussian and then
        convolving should closely match convolving with the Gaussian first and then differentiating.
      </p>
    </div>

    <div class="iteration-belt__header">
      <h3 class="iteration-belt__title">Gaussian smoothing vs. derivative-of-Gaussian</h3>
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">Pause</button>
    </div>

    <div class="iteration-belt__viewport">
      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/DoGx_kernel.png"
            alt="Horizontal derivative-of-Gaussian kernel">
          <figcaption class="iteration-belt__caption">
            Horizontal DoG kernel: a smoothed, antisymmetric pattern that responds to vertical
            edges while attenuating high-frequency noise.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/DoGy_kernel.png"
            alt="Vertical derivative-of-Gaussian kernel">
          <figcaption class="iteration-belt__caption">
            Vertical DoG kernel: rotated counterpart used to detect horizontal edges with the same
            built-in smoothing.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/Ix_smooth_then_diff.png"
            alt="Horizontal derivative after Gaussian smoothing then finite difference">
          <figcaption class="iteration-belt__caption">
            Horizontal gradient from “smooth then difference”: vertical contours of the tripod,
            coat, and buildings remain sharp, but background noise is strongly reduced.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/Iy_smooth_then_diff.png"
            alt="Vertical derivative after Gaussian smoothing then finite difference">
          <figcaption class="iteration-belt__caption">
            Vertical gradient from “smooth then difference”: horizontal structures such as the
            horizon and rooftops dominate the response with fewer speckles in the sky and grass.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/grad_smooth_then_diff.png"
            alt="Gradient magnitude after Gaussian smoothing then finite difference">
          <figcaption class="iteration-belt__caption">
            Gradient magnitude for “smooth then difference”: a clean edge map where strong
            contours are preserved and fine-grained noise has been suppressed by the initial blur.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/Ix_DoG.png"
            alt="Horizontal derivative from derivative-of-Gaussian convolution">
          <figcaption class="iteration-belt__caption">
            Horizontal gradient from DoG convolution: closely matches the smooth-then-differentiate
            result, confirming that the DoG kernel embeds both smoothing and differentiation.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/Iy_DoG.png"
            alt="Vertical derivative from derivative-of-Gaussian convolution">
          <figcaption class="iteration-belt__caption">
            Vertical gradient from DoG convolution: edge strengths and locations align with the
            corresponding smooth-then-difference vertical derivative.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/one/3_derivative_of_gaussian_filter/grad_DoG.png"
            alt="Gradient magnitude from derivative-of-Gaussian convolution">
          <figcaption class="iteration-belt__caption">
            Gradient magnitude from DoG convolution: nearly indistinguishable from the
            smooth-then-difference magnitude, demonstrating the equivalence of the two pipelines in
            both structure and edge strength.
          </figcaption>
        </figure>
      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        The side-by-side comparison shows that Gaussian smoothing before finite differencing and
        direct convolution with derivative-of-Gaussian kernels produce almost identical gradients
        and edge magnitudes. Any small residual differences are due to numerical details rather
        than the underlying operators. This validates the use of DoG filters as a compact,
        one-step implementation of “blur then differentiate,” which is a standard building block
        for robust edge detection.
      </p>
    </div>
  </section>


  <!-- ============================================ 2.1 Image Sharpening (Unsharp Masking) =========================== -->
  <section class="content gold-glass iteration-belt" id="part2-1">
    <h2 class="section-heading">2.1 Image Sharpening with Unsharp Masking</h2>

    <div class="iteration-belt__intro">
      <p>
        This section applies unsharp masking to enhance fine detail in a photograph of the Taj Mahal.
        The method first builds a smoothed version of the image, subtracts it from the original to
        isolate high-frequency structure, and then adds a scaled version of this high-pass signal back
        to the original. The result is a sharpened image in which edges and textures are more crisp,
        while large-scale brightness and color remain stable.
      </p>
    </div>

    <div class="iteration-belt__header">
      <h3 class="iteration-belt__title">Unsharp masking pipeline on the Taj Mahal</h3>
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">Pause</button>
    </div>

    <div class="iteration-belt__viewport">
      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/unsharp_kernel.png"
            alt="Gaussian blur kernel used for unsharp masking">
          <figcaption class="iteration-belt__caption">
            Gaussian blur kernel: a small, isotropic low-pass filter that produces a softened version
            of the image and suppresses high-frequency noise.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/blurred.png" alt="Blurred Taj Mahal image">
          <figcaption class="iteration-belt__caption">
            Blurred image: large-scale structure of the Taj Mahal is preserved, but fine architectural
            details and textures in the trees and walkway are noticeably washed out.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/highpass.png"
            alt="High-pass component obtained by subtracting blur from the original">
          <figcaption class="iteration-belt__caption">
            High-pass component: subtraction of the blurred image from the original isolates edges and
            fine detail. The building outline, ornamental patterns, and strong intensity transitions
            appear as bright responses on a dark background.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/sharpened.png"
            alt="Sharpened Taj Mahal image after unsharp masking">
          <figcaption class="iteration-belt__caption">
            Sharpened result: adding a scaled high-pass signal back to the original yields a crisper
            image. The dome ridges, facade carvings, and tree silhouettes gain local contrast without
            creating strong halos or amplifying background noise.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/sharpened_kernel.png"
            alt="Effective unsharp masking kernel">
          <figcaption class="iteration-belt__caption">
            Effective unsharp kernel: combining the impulse (identity) with a negative Gaussian blur
            produces a single convolution kernel whose positive center and negative surround implement
            “original plus scaled high-pass” in one step.
          </figcaption>
        </figure>
      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        The sequence illustrates how unsharp masking sharpens edges by boosting high-frequency
        structure rather than uniformly increasing global contrast. The Gaussian blur controls which
        spatial scales are treated as detail, the high-pass component encodes that detail explicitly,
        and the final weighting determines how aggressively edges and textures are emphasized while
        keeping the overall appearance of the photograph natural.
      </p>
    </div>

    <div class="iteration-belt__intro">
      <p>
        The same unsharp masking pipeline is applied to a scanned photograph of a grandfather as a
        second example. Here the blur uses a larger 21×21 Gaussian with σ = 4.75 so that the
        high-pass component captures mid-scale structure such as facial features, clothing folds,
        and plant outlines while averaging over film grain and digitization noise.
      </p>
    </div>

    <div class="iteration-belt__header">
      <h3 class="iteration-belt__title">Unsharp masking on a vintage family scan</h3>
      <!-- <button type="button" class="iteration-belt__toggle" aria-pressed="false">Pause</button> -->
    </div>

    <div class="iteration-belt__viewport">
      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/grandpa/unsharp_kernel.png"
            alt="Gaussian blur kernel used to smooth the vintage photo">
          <figcaption class="iteration-belt__caption">
            21×21 Gaussian blur kernel (σ = 4.75) used to define the low-pass component for the
            vintage photograph.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/grandpa/gp.jpeg"
            alt="Original vintage family photograph before sharpening">
          <figcaption class="iteration-belt__caption">
            Original scan: a slightly faded, low-contrast family photograph with modest blur and
            visible texture from aging and digitization.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/grandpa/blurred.png"
            alt="Blurred version of the vintage family photograph">
          <figcaption class="iteration-belt__caption">
            Blurred image: large shapes such as the figure and background plants remain, but fine
            details in the face, shirt, and foliage are smoothed away by the Gaussian filter.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/grandpa/highpass.png"
            alt="High-pass component of the vintage family photograph">
          <figcaption class="iteration-belt__caption">
            High-pass component: subtraction of the blur from the original isolates edges and local
            contrast changes, outlining the figure, clothing seams, and wire fence while largely
            suppressing uniform regions.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/grandpa/sharpened.png"
            alt="Sharpened vintage family photograph after unsharp masking">
          <figcaption class="iteration-belt__caption">
            Sharpened result: adding the high-pass signal back (amount = 1.0) restores crispness to
            facial features and the surrounding vegetation, making the photograph feel clearer
            without introducing strong halos or amplifying background noise.
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/two/1_sharpening/grandpa/sharpened_kernel.png"
            alt="Effective unsharp masking kernel for the vintage family photograph">
          <figcaption class="iteration-belt__caption">
            Effective unsharp kernel: the central impulse and negative Gaussian surround together
            implement the entire “original + scaled high-pass” operation as a single convolution.
          </figcaption>
        </figure>
      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        Compared with the Taj Mahal example, this setting uses a broader blur to respect the softer
        focus and lower resolution of the scan. The unsharp mask selectively boosts mid-frequency
        detail that carries important semantic structure—such as the subject’s expression and
        outline—while keeping film grain and age-related artifacts from becoming visually dominant.
      </p>
    </div>
  </section>




  <script src="../script.js"></script>
</body>

</html>