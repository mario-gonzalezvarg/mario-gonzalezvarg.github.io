<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Project 3B – Stitching Photo Mosaics</title>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Manrope:wght@300;400;600;700&display=swap" rel="stylesheet" />

  <link rel="stylesheet" href="../../styles.css" />
</head>

<body class="top-title">
  <canvas id="sky" aria-hidden="true"></canvas>

  <header class="site-header">
    <div class="content">
      <nav class="top-nav" aria-label="Project navigation">
        <a href="../../index.html" class="nav-logo">Home</a>
      </nav>
    </div>
  </header>

  <div class="content big-title">
    <h1 class="title">Stitching Photo Mosaics</h1>

    <div class="project-switcher" aria-label="Jump to project parts">
      <a href="../parta/part3a.html" class="project-switcher__button">
        Part A
      </a>
      <a href="../partb/part3b.html" class="project-switcher__button project-switcher__button--primary">
        Part B
      </a>
    </div>
  </div>

  <!-- ========== B.1: Corner Detection (Harris + ANMS) ========== -->
  <section class="content gold-glass iteration-belt" id="b1">
    <h2 class="section-heading">B.1 Corner Detection</h2>

    <div class="iteration-belt__intro">
      <p>
        Corner detection can be decomposed into two judgments: where the image contains two-directional variation, and
        which
        of those locations should be retained as a stable basis for later matching. The Harris response supplies the
        first
        judgment by measuring whether local gradients provide evidence of structure in more than one direction; a high
        score
        is interpreted as a location whose appearance is locally informative rather than edge-like or flat. The Harris
        map, taken alone, does not enforce a useful distribution. Dense clusters occur whenever repeated texture
        generates many neighboring pixels with similar scores, so the raw output is often an over-complete description
        of a
        single region. Adaptive Non-Maximal Suppression (ANMS) supplies the second judgment by trading quantity for
        coverage:
        points are retained when they remain strong while also being far from stronger competitors, which spreads the
        set
        across the object and reduces redundancy.
      </p>

    </div>

    <!-- ---------------- Example: g2 (green glass) ---------------- -->
    <h3 class="iteration-belt__row-title" style="margin-top: 0.25rem;">Example 1 (g2 — green glass)</h3>

    <div class="viz-pair" style="--viz-pair-max: 34rem;" role="group"
      aria-label="Harris vs ANMS corner distribution (g2)">
      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Harris (raw)</div>
        <img class="viz-pair__image" src="media/1_corner/g2/corners_harris.png"
          alt="Harris corners for g2 (raw detections)" loading="lazy">
      </figure>

      <figure class="viz-pair__panel">
        <div class="viz-pair__title">ANMS (suppressed)</div>
        <img class="viz-pair__image" src="media/1_corner/g2/corners_anms.png"
          alt="ANMS-selected corners for g2 (spatially distributed subset)" loading="lazy">
      </figure>
    </div>

    <div class="iteration-belt__intro">
      <p>
        The two images in each pair serve as a diagnostic. The Harris visualization indicates the underlying “evidence
        field”
        (where the image claims to have corners), while the ANMS visualization indicates whether that evidence can be
        converted into a spatially balanced set that will not collapse downstream matching into a single textured patch.
      </p>

    </div>

    <div class="iteration-belt__outro">
      <p>
        The green glass sculpture is a difficult corner source because local appearance is less stable. Translucency and
        internal refraction generate view-dependent highlights and low-frequency intensity drift, and darker regions
        reduce
        signal-to-noise; both effects weaken the gradient evidence that Harris relies on. Increasing
        <code>max-harris</code>
        to 10000 preserves more marginal candidates so that ANMS can still extract a reasonably distributed set of 450
        points
        without collapsing onto a few bright highlight boundaries.
      </p>
    </div>

    <!-- ---------------- Example: r1 (red glass) ---------------- -->
    <h3 class="iteration-belt__row-title" style="margin-top: 1.75rem;">Example 2 (r1 — red glass)</h3>

    <div class="viz-pair" style="--viz-pair-max: 34rem;" role="group"
      aria-label="Harris vs ANMS corner distribution (r1)">
      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Harris (raw)</div>
        <img class="viz-pair__image" src="media/1_corner/r1/corners_harris.png"
          alt="Harris corners for r1 (raw detections)" loading="lazy">
      </figure>

      <figure class="viz-pair__panel">
        <div class="viz-pair__title">ANMS (suppressed)</div>
        <img class="viz-pair__image" src="media/1_corner/r1/corners_anms.png"
          alt="ANMS-selected corners for r1 (spatially distributed subset)" loading="lazy">
      </figure>
    </div>

    <div class="iteration-belt__outro">
      <p>
        The red glass sculpture provides stronger, more repeatable corner evidence because the dominant cues are stable
        surface boundaries and consistent shading transitions. The Harris field therefore contains more clearly
        separated
        maxima, so fewer raw candidates (<code>max-harris</code> = 6000) are sufficient, and ANMS can retain a smaller
        set
        (350) while still maintaining broad coverage across the structure.
      </p>
      <p>
        Failures concentrate where the premise of “corner stability” is violated. Specular highlights can create corners
        that move with viewpoint rather than with geometry; repeated micro-texture can produce many near-duplicates that
        remain locally strong but do not add new information; and low-texture regions can leave ANMS with little
        evidence to
        distribute. Success is most reliable when the selected points lie on rigid, high-contrast geometry whose
        gradient
        structure persists under small viewpoint changes.
      </p>
    </div>
  </section>



  <!-- ========== B.2: Feature Descriptors ========== -->
  <section class="content gold-glass iteration-belt" id="b2">
    <h2 class="section-heading">B.2 Feature Descriptors</h2>

    <div class="iteration-belt__intro">
      <p>
        Descriptor extraction can be reduced to three operations: sampling a neighborhood around each retained corner,
        canonicalizing that neighborhood into a fixed-dimensional representation, and comparing representations by a
        distance rule. The purpose of canonicalization is not aesthetic compression, but invariance: a match should be
        driven by local structure that persists across views rather than by absolute exposure or global tone.
      </p>
    </div>

    <div class="code-snippets" aria-label="Descriptor construction summary">
      <div class="code-snippets__title">Descriptor construction</div>
      <details class="code-snippet" open>
        <summary>patch → vector</summary>
        <div class="code-snippet__body">
          <pre class="code-block"><code>1) Extract a square patch around each keypoint (grayscale).
2) Apply mild smoothing (stabilizes against pixel noise / aliasing).
3) Downsample to a small canonical grid (e.g., 8×8) to form a compact template.
4) Normalize per patch (zero-mean, unit-variance) to reduce exposure sensitivity.
5) Flatten to a descriptor vector; compare descriptors by L2 distance.</code></pre>
        </div>
      </details>
    </div>

    <div class="iteration-belt__intro">
      <p>
        Two diagnostic visualizations are used to judge whether the descriptor is behaving as intended. The
        <em>descriptor
          points</em> image shows coverage, whether samples span the object and the overlap region instead of collapsing
        into a
        single cluster. The <em>descriptor patches</em> grid shows discriminability, whether the encoded neighborhoods
        contain
        stable edges/corners or instead devolve into flat, noisy, or highlight-dominated templates that cannot support
        reliable matching.
      </p>
    </div>

    <h3 class="iteration-belt__row-title" style="margin-top: 0.25rem;">Example 1 (g2 — green glass)</h3>
    <div class="viz-pair" style="--viz-pair-max: 34rem;" role="group"
      aria-label="Descriptor points and patches (green glass)">
      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Descriptor points</div>
        <img class="viz-pair__image" src="media/2_descriptor/g2/descriptor_points.png"
          alt="Keypoints where descriptors are extracted on the green glass sculpture" loading="lazy">
      </figure>

      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Descriptor patches</div>
        <img class="viz-pair__image" src="media/2_descriptor/g2/descriptor_patches.png"
          alt="Grid of normalized descriptor patches extracted from the green glass sculpture" loading="lazy">
      </figure>
    </div>

    <div class="iteration-belt__outro">
      <p>
        Again, we emphasize how the green glass case is systematically harder because the local evidence is less
        repeatable. Translucency introduces
        view-dependent highlights and internal refraction, so a patch centered at the “same” geometric location can
        change its
        intensity layout between views. Dark regions additionally reduce signal-to-noise; after per-patch normalization,
        small
        sensor noise can be scaled into meaningful-looking contrast, which makes some templates appear distinctive while
        not
        being stable.
      </p>
      <!-- <p>
      The red glass case is comparatively cooperative because a larger fraction of the neighborhood structure is anchored
      to stable surface boundaries and consistent gradient patterns. The patch grid therefore contains more repeatable
      edge/corner configurations, and the point distribution more often yields keypoints whose descriptors remain coherent
      across viewpoint changes.
    </p>
    <p>
      Descriptor failure concentrates in (1) low-texture neighborhoods that collapse to noise after normalization,
      (2) repeated micro-structure where many patches are genuinely similar and therefore ambiguous, and
      (3) specular/refractive regions where appearance is not attached to the surface. Descriptor success is most reliable
      when the neighborhood contains rigid, high-contrast geometry that preserves its gradient structure across views.
    </p> -->
    </div>

    <h3 class="iteration-belt__row-title" style="margin-top: 1.75rem;">Example 2 (r1 — red glass)</h3>
    <div class="viz-pair" style="--viz-pair-max: 34rem;" role="group"
      aria-label="Descriptor points and patches (red glass)">
      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Descriptor points</div>
        <img class="viz-pair__image" src="media/2_descriptor/r1/descriptor_points.png"
          alt="Keypoints where descriptors are extracted on the red glass sculpture" loading="lazy">
      </figure>

      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Descriptor patches</div>
        <img class="viz-pair__image" src="media/2_descriptor/r1/descriptor_patches.png"
          alt="Grid of normalized descriptor patches extracted from the red glass sculpture" loading="lazy">
      </figure>
    </div>

    <div class="iteration-belt__outro">
      <!-- <p>
      Again, we emphasize how the green glass case is systematically harder because the local evidence is less repeatable. Translucency introduces
      view-dependent highlights and internal refraction, so a patch centered at the “same” geometric location can change its
      intensity layout between views. Dark regions additionally reduce signal-to-noise; after per-patch normalization, small
      sensor noise can be scaled into meaningful-looking contrast, which makes some templates appear distinctive while not
      being stable.
    </p> -->
      <p>
        The red glass case is comparatively cooperative because a larger fraction of the neighborhood structure is
        anchored
        to stable surface boundaries and consistent gradient patterns. The patch grid therefore contains more repeatable
        edge/corner configurations, and the point distribution more often yields keypoints whose descriptors remain
        coherent
        across viewpoint changes.
        <!-- </p>
    <p> -->
        Descriptor failure concentrates in (1) low-texture neighborhoods that collapse to noise after normalization,
        (2) repeated micro-structure where many patches are genuinely similar and therefore ambiguous, and
        (3) specular/refractive regions where appearance is not attached to the surface. Descriptor success is most
        reliable
        when the neighborhood contains rigid, high-contrast geometry that preserves its gradient structure across views.
      </p>
    </div>
  </section>



  <!-- ========== B.3: Feature Matching ========== -->
  <section class="content gold-glass iteration-belt" id="b3">
    <h2 class="section-heading">B.3 Feature Matching</h2>

    <div class="iteration-belt__intro">
      <p>
        Feature matching can be treated as a test of whether the descriptor representation is sufficiently distinctive
        and
        sufficiently repeatable to survive a viewpoint change. Each retained corner produces a descriptor vector; for
        every
        descriptor in Image A, the nearest neighbors in Image B are queried in descriptor space, and a decision rule
        filters
        ambiguous hypotheses before they become geometric constraints.
      </p>
      <p>
        The diagnostic objective is twofold: (1) verify that the selected keypoints and descriptors for the second image
        (r2) contain stable structure rather than highlight noise, and (2) verify that the accepted matches form a
        coherent
        pattern across the overlap rather than a dense set of inconsistent crossings.
      </p>
    </div>

    <h3 class="iteration-belt__row-title" style="margin-top: 0.25rem;">Reference features (r2)</h3>

    <div class="viz-pair" style="--viz-pair-max: 34rem;" role="group" aria-label="r2 corners: Harris vs ANMS">
      <figure class="viz-pair__panel">
        <div class="viz-pair__title">r2 Harris (raw)</div>
        <img class="viz-pair__image" src="media/1_corner/r2/corners_harris.png"
          alt="Harris corner detections for r2 (raw candidates)" loading="lazy">
      </figure>

      <figure class="viz-pair__panel">
        <div class="viz-pair__title">r2 ANMS (suppressed)</div>
        <img class="viz-pair__image" src="media/1_corner/r2/corners_anms.png"
          alt="ANMS-selected corners for r2 (spatially distributed subset)" loading="lazy">
      </figure>
    </div>

    <div class="viz-pair" style="--viz-pair-max: 34rem;" role="group" aria-label="r2 descriptors: points and patches">
      <figure class="viz-pair__panel">
        <div class="viz-pair__title">r2 descriptor points</div>
        <img class="viz-pair__image" src="media/2_descriptor/r2/descriptor_points.png"
          alt="Keypoints where descriptors are extracted on r2" loading="lazy">
      </figure>

      <figure class="viz-pair__panel">
        <div class="viz-pair__title">r2 descriptor patches</div>
        <img class="viz-pair__image" src="media/2_descriptor/r2/descriptor_patches.png"
          alt="Grid of normalized descriptor patches for r2" loading="lazy">
      </figure>
    </div>

    <div class="code-snippets" aria-label="Matching rule and parameters">
      <div class="code-snippets__title">Matching rule</div>

      <details class="code-snippet" open>
        <summary>ratio test (Lowe-style)</summary>
        <div class="code-snippet__body">
          <pre class="code-block"><code>For each descriptor in A:
  find the best and second-best matches in B by L2 distance.
  keep the best match only if (d1 / d2) < ratio.

ratio = 0.8
matches kept = 112</code></pre>
        </div>
      </details>
    </div>

    <figure class="viz-pair__panel" style="max-width: 80rem; margin: 1.5rem auto 0;">
      <div class="viz-pair__title">Filtered feature matches (r1 ↔ r2)</div>
      <img class="viz-pair__image" src="media/3_featureMatch/r12/matches.png"
        alt="Visualization of filtered matches between r1 and r2 with correspondence lines" loading="lazy">
    </figure>

    <div class="iteration-belt__outro">
      <p>
        The match visualization encodes a set of correspondence hypotheses: each line asserts that two local
        neighborhoods,
        one in r1 and one in r2, are similar under the chosen descriptor. Coherence is indicated when many lines connect
        analogous geometric regions and share a consistent global trend (reflecting the dominant camera motion). Failure
        is
        indicated by a large population of crisscrossing lines that jump between unrelated regions, which typically
        occurs
        when descriptors are not distinctive (repeated texture) or not repeatable (specular / refractive changes).
      </p>
      <p>
        The ratio test is a practical ambiguity filter. When the nearest and second-nearest candidates are nearly tied,
        the
        descriptor does not supply strong evidence for a unique match, so the correspondence is rejected. This rejection
        is
        desirable before geometry: even a small fraction of ambiguous matches can dominate later homography estimation.
      </p>
      <p>
        The red glass pair (r1↔r2) is comparatively matchable because many corner neighborhoods are anchored to stable
        surface boundaries and consistent gradient patterns. In contrast, the green translucent case tends to produce
        view-dependent intensity rearrangements inside patches, so the “same” physical region can yield a different
        descriptor and either fail the ratio test (no confident nearest neighbor) or pass spuriously (nearest neighbor
        is
        driven by highlight coincidence rather than geometry).
      </p>
    </div>
  </section>


  <!-- ========== B.4: RANSAC for Robust Homography ========== -->
  <section class="content gold-glass iteration-belt" id="b4">
    <h2 class="section-heading">B.4 RANSAC for Robust Homography</h2>

    <div class="iteration-belt__intro">
      <p>
        Robust homography estimation can be decomposed into two tasks: proposing a projective warp from uncertain
        correspondence evidence, and separating the correspondences that truly support a single warp from those that only
        resemble a match in descriptor space. Feature matching supplies a large set of candidate pairs, yet a single
        homography cannot satisfy all of them when repeated texture, specular structure, or low-contrast patches produce
        geometrically inconsistent matches.
      </p>
      <p>
        RANSAC operationalizes this separation through a consistency test. A candidate homography <em>H</em> predicts where a
        point <em>x</em> from Image A should appear in Image B; the discrepancy to the proposed match <em>x′</em> defines a
        reprojection error. A match is treated as an inlier when this error is below a pixel threshold <em>τ</em>. The final
        estimate is the homography whose implied inlier set is largest, followed by a least-squares refit on that inlier set.
      </p>
    </div>

    <div class="code-snippets" aria-label="RANSAC algorithm for homography">
      <div class="code-snippets__title">RANSAC algorithm (homography)</div>

      <details class="code-snippet" open>
        <summary>minimal sampling + consensus selection</summary>
        <div class="code-snippet__body">
          <pre class="code-block"><code>Inputs:
  matches = {(x_i, x'_i)}  # candidate correspondences in pixel coords
  τ         # inlier threshold (pixels)
  N         # number of random hypotheses

Procedure:
  best_inliers = ∅
  for t in 1..N:
    1) sample 4 correspondences uniformly at random
    2) solve H_t from the 4 pairs (projective DLT + normalization)
    3) for each match i:
         x̂'_i = project(H_t, x_i)          # homogeneous divide
         e_i   = ||x'_i - x̂'_i||_2
         mark i inlier if e_i < τ
    4) keep H_t if it yields more inliers (or equal inliers with smaller mean error)

Return:
  H*       = argmax_t |inliers(H_t)|
  H_final  = refit homography using all inliers of H* (least squares)</code></pre>
        </div>
      </details>
    </div>

    <div class="iteration-belt__outro">
      <p>
        The procedure treats each correspondence as a fallible witness. A single match provides weak evidence for a warp,
        because many incorrect matches can be produced by local appearance similarity. Evidence becomes strong only when a
        large collection of matches exhibits the same “constant conjunction”: their reprojection errors remain small under a
        common <em>H</em>. Outliers fail this conjunction and are excluded by the inlier test.
      </p>
      <p>
        Two parameters control the practical behavior. The threshold <em>τ</em> sets the tolerance for noise (feature
        localization, mild non-planarity, and interpolation) while limiting contamination by incorrect matches. The iteration
        count <em>N</em> sets the probability of ever sampling an all-inlier minimal set; larger <em>N</em> is required when
        the inlier ratio is low. After a consensus set is found, refitting on all inliers reduces sensitivity to the random
        sample and concentrates the final estimate on the most coherent evidence.
      </p>
      <p>
        Seam quality in the final mosaic is therefore a geometric diagnostic rather than a blending artifact. When the warp is
        slightly wrong, the overlap contains two displaced copies of the same edges; feathering cannot erase duplicated
        structure and instead reveals a boundary. When the warp is supported by a tight inlier set, feathering can act on
        illumination differences alone, spreading residual exposure mismatch across a wide band rather than localizing it.
      </p>
    </div>

    <!-- ============================ r1 ↔ r2: manual vs automatic ============================ -->
    <h3 class="iteration-belt__row-title" style="margin-top: 0.25rem;">
      Red glass (r1 ↔ r2): manual vs automatic
    </h3>

    <div class="iteration-belt__intro">
      <p>
        The manual pipeline supplies a small number of correspondences chosen by inspection; the automatic pipeline supplies a
        larger set of correspondences chosen by descriptor similarity and then filtered by RANSAC. The comparison is intended
        to isolate the effect of outlier rejection: both mosaics apply the same projective warp model, yet only the automatic
        method explicitly tests global geometric consistency across many candidates.
      </p>
    </div>

    <div class="viz-pair" style="--viz-pair-max: 36rem;" role="group" aria-label="Manual correspondences visualization (r1 to r2)">
      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Manual points (Image A)</div>
        <img class="viz-pair__image" src="media/4_ransac/r2/manual/a2b_points_imgA.png"
          alt="Visualization of manually selected correspondence points in image A" loading="lazy">
      </figure>

      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Manual points (Image B)</div>
        <img class="viz-pair__image" src="media/4_ransac/r2/manual/a2b_points_imgB.png"
          alt="Visualization of manually selected correspondence points in image B" loading="lazy">
      </figure>
    </div>

    <div class="code-snippets" aria-label="r1/r2 runtime and fit statistics">
      <div class="code-snippets__title">Runtime and fit statistics (r1 ↔ r2)</div>

      <details class="code-snippet" open>
        <summary>Manual (hand correspondences)</summary>
        <div class="code-snippet__body">
          <pre class="code-block"><code>pairs clicked: 12
time_click_s:       246.424675
time_computeH_s:      0.000235
time_total_s:       246.980528

reprojection_mean_px: 7.730348
reprojection_rms_px:  8.543990</code></pre>
        </div>
      </details>

      <details class="code-snippet" open>
        <summary>Automatic (matches + RANSAC)</summary>
        <div class="code-snippet__body">
          <pre class="code-block"><code>ransac_iters:       2000
ransac_thresh_px:     3.0
feather_px:          40
inliers_refined:     41
mean_inlier_err_px:   1.327582
time_total_s:         9.231406</code></pre>
        </div>
      </details>
    </div>

    <div class="viz-pair" style="--viz-pair-max: 36rem;" role="group" aria-label="Final mosaic comparison (r1 to r2)">
      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Manual (hand correspondences)</div>
        <img class="viz-pair__image" src="media/4_ransac/r2/manual/r1_r2_mosaic.png"
          alt="Manual mosaic from hand-picked correspondences" loading="lazy">
      </figure>

      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Automatic (matches + RANSAC + feather)</div>
        <img class="viz-pair__image" src="media/4_ransac/r2/auto/pano.png"
          alt="Automatic mosaic from feature matches filtered by RANSAC with feather blending" loading="lazy">
      </figure>
    </div>

    <div class="iteration-belt__outro">
      <p>
        The visual difference aligns with the error statistics. A mean reprojection error near several pixels implies that
        prominent edges in the overlap are effectively doubled; the seam is therefore forced to display a transition between
        two misregistered copies. After RANSAC filtering, the inlier error drops to the low-pixel regime, so the overlap is
        geometrically coherent and blending acts primarily on exposure mismatch.
      </p>
    </div>

    <!-- ============================ Catalina: manual vs automatic ============================ -->
    <h3 class="iteration-belt__row-title" style="margin-top: 2.25rem;">
      Catalina (IMG_1694 ↔ IMG_1697): manual vs automatic
    </h3>

    <div class="viz-pair" style="--viz-pair-max: 36rem;" role="group" aria-label="Manual correspondences visualization (Catalina)">
      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Manual points (Image A)</div>
        <img class="viz-pair__image" src="media/4_ransac/catalina/manual/imgA.png"
          alt="Visualization of manually selected correspondence points in the first Catalina image" loading="lazy">
      </figure>

      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Manual points (Image B)</div>
        <img class="viz-pair__image" src="media/4_ransac/catalina/manual/imgB.png"
          alt="Visualization of manually selected correspondence points in the second Catalina image" loading="lazy">
      </figure>
    </div>

    <figure class="viz-pair__panel" style="max-width: 80rem; margin: 1.5rem auto 0;">
      <div class="viz-pair__title">RANSAC inlier matches (IMG_1694 ↔ IMG_1697)</div>
      <img class="viz-pair__image" src="media/4_ransac/catalina/auto/IMG_1694_IMG_1697_auto_inlier_matches.png"
        alt="Visualization of RANSAC-filtered inlier matches between the Catalina image pair" loading="lazy">
    </figure>

    <div class="code-snippets" aria-label="Catalina runtime comparison (manual sequence vs auto pair)">
      <div class="code-snippets__title">Runtime comparison (Catalina)</div>

      <details class="code-snippet" open>
        <summary>Manual (mosaic-seq)</summary>
        <div class="code-snippet__body">
          <pre class="code-block"><code>command: mosaic-seq
n_imgs: 7
ref_idx: 3
pairs_per_neighbor: 12
feather_px: 60
time_total_s: 102.002648</code></pre>
        </div>
      </details>

      <details class="code-snippet" open>
        <summary>Automatic (b4-autostitch-pair)</summary>
        <div class="code-snippet__body">
          <pre class="code-block"><code>ransac_iters: 2000
ransac_thresh_px: 3.0
feather_px: 60
inliers_refined: 22
mean_inlier_err_px: 1.7998
time_total_s: 24.949266</code></pre>
        </div>
      </details>
    </div>

    <div class="viz-pair" style="--viz-pair-max: 36rem;" role="group" aria-label="Final mosaic comparison (Catalina)">
      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Manual (hand correspondences)</div>
        <img class="viz-pair__image" src="media/4_ransac/catalina/manual/pano.png"
          alt="Manual mosaic from hand-picked correspondences (Catalina)" loading="lazy">
      </figure>

      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Automatic (matches + RANSAC + feather)</div>
        <img class="viz-pair__image" src="media/4_ransac/catalina/auto/ransac_crop.png"
          alt="Automatic mosaic from feature matches and RANSAC with feather blending (Catalina)" loading="lazy">
      </figure>
    </div>

    <div class="iteration-belt__outro">
      <p>
        The Catalina comparison also illustrates error accumulation. The manual result is produced from a longer sequence of
        warps, so small local inaccuracies can compound into a global disagreement that becomes visible as a boundary. The
        automatic pair fit is constrained by an explicit inlier test on a dense candidate set, which concentrates the warp on
        the overlap that is most mutually consistent.
      </p>
    </div>

    <!-- ============================ Canyon: manual vs automatic ============================ -->
    <h3 class="iteration-belt__row-title" style="margin-top: 2.25rem;">
      Canyon (IMG_7140 ↔ IMG_7141): manual vs automatic
    </h3>

    <figure class="viz-pair__panel" style="max-width: 80rem; margin: 1.25rem auto 0;">
      <div class="viz-pair__title">RANSAC inlier matches (IMG_7140 ↔ IMG_7141)</div>
      <img class="viz-pair__image" src="media/4_ransac/canyon/auto/IMG_7140_IMG_7141_auto_inlier_matches.png"
        alt="Visualization of RANSAC-filtered inlier matches between the canyon image pair" loading="lazy">
    </figure>

    <div class="code-snippets" aria-label="Canyon runtime comparison (manual vs auto)">
      <div class="code-snippets__title">Runtime comparison (Canyon)</div>

      <details class="code-snippet" open>
        <summary>Manual (hand correspondences)</summary>
        <div class="code-snippet__body">
          <pre class="code-block"><code>command: mosaic-pair
pairs: 12
feather_px: 40
time_total_s: 17.222547</code></pre>
        </div>
      </details>

      <details class="code-snippet" open>
        <summary>Automatic (matches + RANSAC)</summary>
        <div class="code-snippet__body">
          <pre class="code-block"><code>ransac_iters: 20000
ransac_thresh_px: 8.0
feather_px: 90
inliers_refined: 44
mean_inlier_err_px: 2.583541
time_total_s: 12.646754</code></pre>
        </div>
      </details>
    </div>

    <div class="viz-pair" style="--viz-pair-max: 36rem;" role="group" aria-label="Final mosaic comparison (Canyon)">
      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Manual (hand correspondences)</div>
        <img class="viz-pair__image" src="media/4_ransac/canyon/manual/blended.png"
          alt="Manual canyon mosaic produced from hand-picked correspondences" loading="lazy">
      </figure>

      <figure class="viz-pair__panel">
        <div class="viz-pair__title">Automatic (matches + RANSAC + feather)</div>
        <img class="viz-pair__image" src="media/4_ransac/canyon/auto/IMG_7140_IMG_7141_auto.png"
          alt="Automatic canyon mosaic produced from feature matches filtered by RANSAC with feather blending" loading="lazy">
      </figure>
    </div>

    <div class="iteration-belt__outro">
      <p>
        The canyon scene contains broad low-frequency regions (sky and haze) where descriptors are less distinctive, so the
        role of the inlier test is especially visible: only matches that agree with a single projective motion are retained.
        When that agreement is enforced, the remaining seam is governed mainly by exposure and atmospheric variation rather
        than by duplicated edge geometry.
      </p>
    </div>

  </section>


  <script src="../../script.js" defer></script>
</body>

</html>