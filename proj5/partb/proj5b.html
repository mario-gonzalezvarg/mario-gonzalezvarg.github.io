<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Flow Matching from Scratch – Part B</title>
  <link rel="stylesheet" href="../styles.css" />
</head>

<!-- <body class="top-title"> -->

<body class="top-title theme-b">

  <canvas id="sky" aria-hidden="true"></canvas>

  <!-- Top nav bar -->
  <header class="site-header">
    <div class="content">
      <nav class="top-nav" aria-label="Project navigation">
        <a href="../../index.html" class="nav-logo">
          Home
        </a>
        <!-- <div class="top-nav__links">
          <a href="../parta/proj5a.html" class="nav-link">Part A</a>
          <a href="../partb/proj5b.html" class="nav-link">Part B</a>
        </div> -->
      </nav>
    </div>
  </header>

  <!-- Title + Part A / Part B buttons -->
  <div class="content big-title">
    <h1 class="title">Flow Matching</h1>

    <div class="project-switcher" aria-label="Jump to project parts">
      <a href="../parta/proj5a.html" class="project-switcher__button project-switcher__button--primary">
        Part A
      </a>
      <a href="../partb/proj5b.html" class="project-switcher__button">
        Part B
      </a>
    </div>
  </div>

  <!-- ===================================================== Part 1: Single-Step Denoising UNet ===================================================== -->
  <section class="content gold-glass writing-section">
    <h2 class="section-heading">Single-Step Denoising UNet</h2>

    <p>
      The UNet architecture can be decomposed into three components: a left encoder path that applies
      a sequence of downsampling blocks to shrink spatial resolution and expand channel dimensionality,
      a compact middle block that aggregates global context, and a right decoder path that restores
      spatial resolution through upsampling blocks. Skip connections transmit feature maps from each
      encoder stage to the matching decoder stage, ensuring that coarse semantic structure from deeper
      layers is combined with fine spatial detail preserved in earlier layers.
    </p>

    <figure class="unet-diagram">
      <img src="media/diagrams/unconditional_arch.png" alt="UNet architecture diagram" />
    </figure>

    <p>
      In the MNIST setting, this template is instantiated at three resolution levels. A small vocabulary
      of operations—convolutions with Batch Normalization and GELU activations, strided convolutions for
      downsampling, transposed convolutions for upsampling, flattening via average pooling, and
      channel-wise concatenation—is composed into the <code>ConvBlock</code>, <code>DownBlock</code>, and
      <code>UpBlock</code> modules shown below. During Part&nbsp;1, this UNet functions as a single-step
      denoiser: a clean 1×28×28 MNIST image is corrupted by Gaussian noise with standard deviation σ, and
      the network is trained with an MSE objective to predict the clean digit in a single forward pass.
    </p>

    <figure class="unet-diagram">
      <img src="media/diagrams/atomic_ops_new.png"
        alt="Atomic operations composing ConvBlock, DownBlock, and UpBlock" />
    </figure>

    <p>
      Varying the corruption level provides a controlled way to examine the limits of the model. Small
      σ produces mildly perturbed digits, whereas large σ yields images that approach pure noise. This
      parameter sweeps out the regimes in which recognizable structure persists and the regimes in which
      the network must effectively infer the digit from highly degraded evidence.
    </p>
  </section>


  <!-- ===================================================== Part 1.2: Using the UNet to Train a Denoiser ===================================================== -->
  <!-- ===================================================== Part 1.2: Using the UNet to Train a Denoiser ===================================================== -->

  <section class="content gold-glass writing-section">
    <h2 class="section-heading">1.2 Using the UNet to Train a Denoiser</h2>

    <p>
      The first visualization surveys how a fixed handwritten “7” responds to increasing noise levels.
      As σ moves from 0.0 to 1.0, sparse perturbations evolve into dense, irregular patterns that
      progressively obscure stroke geometry. Mid-range σ values maintain enough brightness structure for
      the digit to remain identifiable, whereas high σ values leave only faint residual cues. This
      sequence delineates the threshold beyond which the representation no longer contains usable
      information.
    </p>

    <figure>
      <img src="media/one/1.2/noising_process.png" alt="Sigma sweep for a fixed MNIST digit" />
      <figcaption>
        Noise-only σ sweep: rising noise gradually eliminates stroke structure until the image becomes
        visually indistinguishable from Gaussian noise.
      </figcaption>
    </figure>

    <p>
      A more detailed study compares the noisy input, the UNet’s reconstruction, and a pixelwise
      absolute-error heatmap at each σ. When σ is small, the reconstruction nearly matches the clean
      digit, and the error map is almost blank. At moderate σ, the network restores a coherent and
      well-formed “7” even though the input is heavily degraded, with remaining error concentrated
      around stroke boundaries where handwritten digits naturally vary. At the highest σ levels,
      reconstruction becomes increasingly speculative, and the error maps broaden to reflect the loss of
      reliable structure in the input.
    </p>

    <figure>
      <img src="media/one/1.2/single_digit_sigma_sweep.gif"
        alt="Noisy input, reconstruction, and error for varying sigma" />
      <figcaption>
        σ-sweep showing noisy input, denoised output, and reconstruction error. The UNet performs
        reliably while structural cues remain, but errors intensify as σ removes nearly all evidence of
        the underlying digit.
      </figcaption>
    </figure>

    <p>
      The final visualization extends this analysis from a single “7” to a small batch of digits. For
      each σ, columns correspond to different MNIST examples, while rows show aligned views such as
      noisy inputs and their corresponding denoised outputs. At low σ the three rows are nearly
      indistinguishable from the clean targets, confirming that mild corruption is corrected with
      minimal distortion. As σ increases, the noisy row becomes dominated by noise, yet the denoised
      row continues to display sharp and legible digits across a range of classes, illustrating that the
      behavior observed on the single “7” generalizes to a broader subset of the dataset.
    </p>

    <figure>
      <img src="media/one/1.2/digit_denoising_sigma_sweep.gif"
        alt="Multi-digit sigma sweep showing noisy and denoised digits for several classes" />
      <figcaption>
        Multi-digit σ-sweep: across several digit classes, the UNet consistently recovers clean,
        class-consistent shapes from increasingly corrupted inputs, mirroring the trends observed in
        the single-digit analysis.
      </figcaption>
    </figure>

    <!-- ============================ 1.2.2 Out-of-Distribution Noise Levels ============================ -->

    <h3 class="section-subheading">1.2.2 Out-of-Distribution Noise Levels</h3>

    <p>
      The denoiser is trained at a single noise level, σ = 0.5, but then evaluated across the full
      range σ ∈ [0.0, 1.0]. Several aggregated statistics summarize how performance degrades as the
      test noise moves away from the training regime and how the network allocates its corrections
      across the image plane.
    </p>
    <!-- 
    <p>
      A first summary compresses each denoised image to a one-dimensional profile by recording the
      average intensity along a central row and stacking these profiles as σ varies. The resulting
      heatmap shows a bright vertical band where the stroke of the digit typically passes and dark
      regions elsewhere. For small σ, the band is narrow and well defined, indicating a stable and
      localized reconstruction of the stroke. As σ increases, this band broadens slightly and weak
      activations appear in neighboring columns, reflecting the model’s growing uncertainty about the
      exact stroke location when the input is heavily corrupted.
    </p> -->

    <figure>
      <img src="media/one/1.2/sigma_row_heatmap.png"
        alt="Row intensity vs sigma for a central scanline of the denoised digit" />
      <figcaption>
        Row intensity vs σ: a central scanline through the denoised digits remains sharply localized at
        low σ and gradually broadens as noise increases, indicating growing uncertainty in stroke
        placement.
      </figcaption>
    </figure>



    <p>
      The class-wise MSE curves provide a complementary view. For each digit class, the test MSE rises
      monotonically with σ, producing a family of smooth, upward-sloping curves. Simple digits with a
      single dominant stroke, such as “1”, incur the lowest error across the range, while more complex
      shapes show higher error, especially at large σ. This pattern reflects the fact that recovering a
      simple vertical stroke from noise is easier than reconstructing multi-stroke digits whose
      structure is more sensitive to small perturbations.
    </p>

    <figure>
      <img src="media/one/1.2/classwise_mse_vs_sigma.png" alt="Class-wise MSE as a function of sigma" />
      <figcaption>
        Class-wise MSE vs σ: errors increase smoothly with noise level, with simple digits such as “1”
        remaining easiest to reconstruct and more complex digits accumulating higher error at large σ.
      </figcaption>
    </figure>

    <!-- <p>
      Averaging the absolute reconstruction error pixelwise over the entire test set at σ = 0.5
      reveals a spatial structure that closely resembles a blurred template of the MNIST manifold.
      Background regions, which are almost always blank, exhibit near-zero error. In contrast, the
      central vertical band and surrounding arcs—where strokes from many digits tend to pass—show
      systematically elevated error. This behavior indicates that the network is highly confident on
      background pixels but inevitably uncertain about the precise thickness and placement of
      handwritten strokes, which vary from digit to digit.
    </p>

    <figure>
      <img src="media/one/1.2/mean_error_heatmap_sigma0.5.png"
        alt="Mean absolute error heatmap over the dataset at sigma 0.5" />
      <figcaption>
        Mean absolute error at σ = 0.5: errors concentrate in regions occupied by strokes across many
        digits, while the background is reconstructed with almost no error.
      </figcaption>
    </figure> -->

    <p>
      Distributional and spectral analyses tell a consistent story. Histograms of pixel intensities
      show that the noisy images spread mass across a broad range of gray levels, while the denoised
      outputs return mass to the sharp bimodal structure characteristic of clean MNIST digits
      (dominant peaks near 0 and 1). In the Fourier domain, magnitude spectra and radial profiles
      reveal that noise injects excess high-frequency energy, whereas the denoiser restores a spectrum
      closely aligned with the clean reference by suppressing high frequencies and preserving
      low-frequency content that encodes overall digit shape.
    </p>

    <figure>
      <img src="media/one/1.2/intensity_histograms.png"
        alt="Pixel intensity histograms for clean, noisy, and denoised digits" />
      <figcaption>
        Pixel intensity distributions: noise flattens the histogram, while the denoised outputs recover
        a distribution that closely matches the clean digits with mass concentrated near background and
        stroke intensities.
      </figcaption>
    </figure>

    <figure>
      <img src="media/one/1.2/frequency_spectra.png"
        alt="2D frequency magnitude maps for clean, noisy, and denoised digits" />
      <figcaption>
        2D frequency spectra: noisy images exhibit amplified high-frequency content, whereas denoised
        images suppress these components and restore a spectrum dominated by low frequencies, similar
        to the clean digits.
      </figcaption>
    </figure>

    <figure>
      <img src="media/one/1.2/frequency_radial.png"
        alt="Radial frequency spectra for clean, noisy, and denoised digits" />
      <figcaption>
        Radial frequency profiles: the noisy curve sits above the clean baseline at high spatial
        frequencies, while the denoised curve tracks the clean profile, confirming that the UNet
        behaves as a data-aware low-pass filter.
      </figcaption>
    </figure>

    <p>
      Finally, the internal representation learned by the network is visualized by projecting
      bottleneck features onto their first two principal components. Points corresponding to different
      digit classes form overlapping but structured clusters, with certain digits (notably “1”) forming
      well-separated lobes. Together with a panel contrasting the lowest- and highest-error examples,
      this view shows that the UNet organizes digits into a meaningful latent space: canonical,
      easy-to-read instances concentrate in regions where the model reconstructs accurately, while
      atypical handwriting styles occupy regions associated with larger reconstruction error.
    </p>

    <figure>
      <img src="media/one/1.2/bottleneck_pca.png" alt="PCA of bottleneck features colored by digit class" />
      <figcaption>
        Bottleneck PCA colored by digit label: digits cluster by class in the latent space, with
        well-formed examples occupying dense regions and more ambiguous handwriting pushed toward the
        periphery.
      </figcaption>
    </figure>


    <figure>
      <img src="media/one/1.2/best_worst_examples_sigma0.5.png"
        alt="Panel of best and worst reconstructed digits at sigma 0.5" />
      <figcaption>
        Best- and worst-case reconstructions at σ = 0.5: simple, cleanly written digits yield very
        small errors, while atypical shapes and ambiguous strokes produce noticeably larger
        reconstruction discrepancies.
      </figcaption>
    </figure>

    <p>
      The sample grids make this evolution visible. Each column corresponds to a test digit, with the
      top row showing the clean target, the middle row a pure-noise input, and the bottom row the
      UNet output. After a single epoch, the outputs resemble blurry, almost class-agnostic blobs:
      rough digit silhouettes are present, but many columns look similar to one another. This
      behavior is consistent with the early steep loss drop: the network quickly learns to map
      arbitrary noise to an “average MNIST digit” that already reduces MSE substantially, without
      yet encoding class-specific structure.
    </p>

    <figure>
      <img src="media/one/1.2/pure_noise_epoch1.png"
        alt="Pure-noise denoiser outputs after epoch 1: clean targets, noise inputs, and blurry outputs" />
      <figcaption>
        Pure-noise denoiser after epoch&nbsp;1: outputs are diffuse, digit-shaped blobs that move the
        prediction away from white noise but remain only weakly tied to the underlying class labels.
      </figcaption>
    </figure>

    <p>
      By epoch&nbsp;5, the bottom row remains qualitatively unchanged. The predictions are still
      smooth, averaged digit prototypes that do not track the identity or fine stroke patterns of
      the targets in the top row; at most, contrast and overall mass are slightly adjusted. This
      stagnation reflects the limitation of the pure-noise setting: because the input carries no
      information about which digit should be produced, the network can only learn a generic digit
      prior rather than accurate, sample-specific reconstructions.
    </p>

    <figure>
      <img src="media/one/1.2/pure_noise_epoch5.png"
        alt="Pure-noise denoiser outputs after epoch 5: clean targets, noise inputs, and similarly blurred outputs" />
      <figcaption>
        Pure-noise denoiser after epoch&nbsp;5: outputs remain blurry, class-agnostic prototype
        digits, showing little visible improvement over epoch&nbsp;1 and confirming that further
        training cannot recover per-sample detail from noise-only inputs.
      </figcaption>
    </figure>


    <p>
      Averaging the absolute reconstruction error pixelwise over the entire test set at σ = 0.5
      reveals a spatial structure that closely resembles a blurred template of the MNIST manifold.
      Background regions, which are almost always blank, exhibit near-zero error. In contrast, the
      central vertical band and surrounding arcs—where strokes from many digits tend to pass—show
      systematically elevated error. This pattern indicates that the network has effectively learned
      a per-pixel average over the dataset: pixels that are consistently off are reconstructed with
      high confidence, whereas pixels that sometimes contain a stroke incur unavoidable residual
      error. The blurred “average digit” visible in the heatmap explains why, even by epoch&nbsp;5 in
      the pure-noise setting, the model converges to soft prototype digits rather than sharpening to
      sample-specific reconstructions.
    </p>

    <figure>
      <img src="media/one/1.2/mean_error_heatmap_sigma0.5.png"
        alt="Mean absolute error heatmap over the dataset at sigma 0.5" />
      <figcaption>
        Mean absolute error at σ = 0.5: errors concentrate in stroke-bearing regions and trace out a
        blurred digit template, showing that the network behaves like a per-pixel average over the
        MNIST dataset and matching the prototype-like outputs observed for pure-noise inputs even
        after epoch&nbsp;5.
      </figcaption>
    </figure>
  </section>

  <!-- ============================ 2.2–2.3 Training and Sampling ============================ -->
  <section class="content gold-glass writing-section">
    <h3 class="section-heading">2.2–2.3 Training and Sampling from the Time-Conditioned UNet</h3>

    <p>
      The time-conditioned UNet is trained on MNIST to predict the instantaneous update that moves a
      noisy digit a little closer to the data manifold. Each training step draws a clean digit,
      samples a random noise level, corrupts the image, and asks the network to predict the
      corresponding flow. The model uses the same convolutional backbone as in Part&nbsp;1, with an
      additional branch that embeds the scalar timestep and injects it into the feature maps. Training
      with Adam, a learning rate of 0.01, batch size 128, and an exponentially decaying schedule over
      15 epochs produces a loss curve with a steep initial drop followed by a long, flat tail,
      indicating rapid learning of a coarse flow field and then gradual refinement of small residual
      errors.
    </p>

    <figure>
      <img src="media/b/2.2/time_unet_train_loss.png" alt="Training loss curve for the time-conditioned UNet" />
      <figcaption>
        Training loss for the time-conditioned UNet over 15 epochs. The loss collapses within the
        first few hundred iterations, then decreases slowly as the learned flow is polished with
        additional passes through the data.
      </figcaption>
    </figure>

    <p>
      Sampling uses this learned flow for iterative denoising. Starting from pure Gaussian noise,
      the sampler marches through a sequence of timesteps, applying the UNet at each step to nudge
      samples toward the digit manifold. The grids below show independent samples at epoch&nbsp;1 and
      epoch&nbsp;20 using the same sampling schedule. After one epoch, most digits are only partially
      formed: strokes are broken, many shapes are ambiguous, and some samples resemble textured
      noise more than digits. By epoch&nbsp;20, the samples have somewhat sharp, continuous strokes, clear
      digit identities, and far fewer artifacts. This progression mirrors the behavior of the loss
      curve: once the coarse flow has been learned, continued training primarily improves local
      geometry and stroke-level detail.
    </p>

    <figure>
      <div class="figure-row">
        <div>
          <img src="media/b/2.2/time_unet_samples_epoch1.png"
            alt="Samples from the time-conditioned UNet after epoch 1" />
          <div class="figure-row__label">Epoch&nbsp;1 samples</div>
        </div>
        <div>
          <img src="media/b/2.2/time_unet_samples_epoch15.png"
            alt="Samples from the time-conditioned UNet after epoch 15" />
          <div class="figure-row__label">Epoch&nbsp;20 samples</div>
        </div>
      </div>
      <figcaption>
        Samples from the time-conditioned UNet early and late in training. After a single epoch the
        model produces noisy, fragmented digits, while by epoch&nbsp;15 the same sampler generates
        clean, class-consistent digits with well-defined strokes.
      </figcaption>
    </figure>


    <p>
      The illsutration below makes this evolution more explicit by tracking a fixed batch of noise seeds
      across epochs. Early in training, trajectories wander and often collapse to distorted or
      inconsistent shapes. As the flow improves, the same seeds converge reliably to stable,
      recognizable digits, illustrating how the learned dynamics gradually carve out the MNIST
      manifold from an initially random latent space.
    </p>

    <figure>
      <img src="media/b/2.2/time_unet_training.gif"
        alt="GIF of iterative denoising with the time-conditioned UNet across epochs" />
      <figcaption>
        Evolution of iterative denoising for fixed noise seeds over training. The trajectories
        transition from unstable, noisy shapes to stable, high-quality digits as the time-conditioned
        UNet converges.
      </figcaption>
    </figure>
  </section>





  <script src="../script.js" defer></script>
</body>

</html>