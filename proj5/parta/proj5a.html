<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Diffusion Models – Visualizations</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Manrope:wght@300;400;600;700&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="../styles.css" />
</head>

<body class="top-title">

  <canvas id="sky" aria-hidden="true"></canvas>
  <header class="site-header">
    <div class="content">
      <nav class="top-nav" aria-label="Project navigation">
        <a href="../../index.html" class="nav-logo">
          Home
        </a>
      </nav>
    </div>
  </header>

  <div class="content big-title">
    <h1 class="title">Diffusion Models</h1>

    <div class="project-switcher" aria-label="Jump to project parts">
      <a href="../parta/proj5a.html" class="project-switcher__button project-switcher__button--primary">
        Part A
      </a>
      <a href="../partb/proj5b.html" class="project-switcher__button">
        Part B
      </a>
    </div>
  </div>

  <!--============================================= HuggingFace Interface Introduction ============================================ -->

  <section class="content gold-glass writing-section">
    <h2 class="section-heading">HuggingFace Model Interface</h2>
    <figure class="writing-section figure img">
      <img src="media/diagrams/pipeline.png" alt="High-level inference pipeline from raw text to predictions.">
    </figure>
    <p>
      The HuggingFace inference pipeline can be generalized as a sequence of clearly defined
      transformations. The process starts with <em>raw text</em>, a plain string such as “This course is
      amazing.” The <strong>tokenizer</strong> takes this string, splits it into subword units, and looks
      up each unit in a fixed vocabulary table. This step produces a sequence of integers called
      <em>input IDs</em>, which is the form the model can consume. The <strong>model</strong> then takes
      these input IDs and computes a real-valued score for each possible label; these scores are called
      <em>logits</em> and encode how compatible the text is with each class before any normalization.
      The final <strong>post-processing</strong> block converts logits into something usable: a softmax
      turns them into probabilities, and simple decision logic (such as taking the largest probability)
      chooses the predicted label, for example POSITIVE versus NEGATIVE sentiment.
    </p>
  </section>


  <section class="content gold-glass writing-section">
    <h2 class="section-heading">Transformer Architecture & DeepFloyd IF</h2>
    <p>
      The internal structure of the model can be decomposed into an embedding layer, a stack of
      transformer layers, and a classification head. First, an <strong>embedding layer</strong> maps each
      input ID to a dense vector that represents the token in a continuous space. A stack of
      <strong>transformer layers</strong> then processes the entire sequence: each layer applies
      self-attention, so every token can use information from every other token, followed by small
      feed-forward networks that refine these combined representations. After several such layers, the
      model extracts a single summary vector for the whole sequence (for example, by reading a special
      classification token or averaging all token vectors). A small <strong>classification head</strong>,
      typically one or two linear layers, takes this summary vector and produces the logits. These logits
      feed directly into the post-processing step from the first paragraph, completing the logical chain
      from raw text to final prediction.
    </p>

    <figure class="writing-section figure img">
      <img src="media/diagrams/transformer_and_head-dark.svg"
        alt="Transformer backbone and output head used in the HuggingFace model.">
    </figure>

    <p>
      DeepFloyd IF can be understood as coupling this text encoder to a three-stage cascaded diffusion
      model: a 64×64 base denoiser (stage 1) followed by two upsamplers that refine the image to higher
      resolutions (stages 2 and 3). For all experiments in this section, prompt embeddings produced by
      the transformer are used as conditioning signals at every denoising step, while the random seed is
      fixed to 12152000 so that differences in the outputs primarily reflect the prompts and sampling
      hyperparameters. Classifier-free guidance is applied with a relatively strong guidance scale in
      stage 1 (γ ≈ 9.0) and slightly lower guidance in the upsampling stages (γ ≈ 7.0), which stabilizes
      global structure while allowing the higher-resolution stages to sharpen details. This setup
      provides a controlled environment for the prompt-based images that follow, making it possible to
      interpret how changes in text descriptions translate into systematic changes in composition,
      lighting, and reflective structure.
    </p>
  </section>


  <!--============================================== 0 DeepFloyd Prompts =========================================================== -->
  <section class="content gold-glass iteration-belt" aria-label="DeepFloyd prompts">
    <h2 class="section-heading">DeepFloyd Prompts</h2>
    <div class="iteration-belt__intro">
      <p>
        The prompt study evaluates how faithfully DeepFloyd IF translates
        language into images, rather than simply demonstrating that it can produce visually pleasing
        samples. A small bank of prompts is first encoded with the HuggingFace text encoder to obtain a
        fixed set of embeddings that are reused throughout the project. From this bank, three prompts are
        chosen as exemplars and are displayed side by side with their generated images, using a single
        random seed and at least two different values of <em>num_inference_steps</em> for each prompt. This
        setup makes it possible to isolate how the model handles semantics: the outputs are examined for
        whether the core subject, viewpoint, and stylistic cues match the description, and whether more
        delicate relational clauses—such as instructions about reflections, grids of windows, or localized
        details—are preserved, softened, or ignored. Across these examples, the model reliably captures
        the broad scene type, composition, and lighting implied by the caption, while showing weaker
        control over complex relational language, which provides a reference point for interpreting the
        more advanced experiments that follow.
      </p>

    </div>

    <div class="iteration-belt__header">
      <!-- <h3 class="iteration-belt__title">Renders</h3> -->
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">Pause</button>
    </div>

    <div class="iteration-belt__viewport">
      <h4 class="iteration-belt__row-title">Timesteps</h4>
      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/prompts/lewis.png">
          <figcaption class="iteration-belt__caption">"a symbolist painting of Lewis and Clark reaching the Columbia
            River, soft edges"</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/prompts/market.png">
          <figcaption class="iteration-belt__caption">"a baroque oil painting of a crowded market lit by torches and
            deep shadows"
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/prompts/mermaid.png">
          <figcaption class="iteration-belt__caption">"a romanticism oil painting of a mermaid seen only as a
            reflection
            in dark waves under moonlight"
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/prompts/glass_planes.png">
          <figcaption class="iteration-belt__caption">" “a renaissance-style painting of a city square reflected in
            many
            small glass panes”
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/prompts/hiroshima.png">
          <figcaption class="iteration-belt__caption">"a symbolist oil painting, close-up of a human eye whose iris is
            a
            reflection of a distant mushroom of the atomic bomb dropped at Hiroshima"
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/prompts/windows.png">
          <figcaption class="iteration-belt__caption">"a realism painting of a library where each window opens onto a
            different sky"
          </figcaption>
        </figure>
      </div>
    </div>

    <!-- <div class="iteration-belt__outro">
    </div> -->

  </section>

  <!-- ============================================ 1.1 Implementing the Forward Process =================================================  -->

  <section class="content gold-glass iteration-belt">
    <h2 class="section-heading">1.1 Forward Process</h2>
    <div class="iteration-belt__intro">
      <p>
        The forward diffusion process is implemented as a function <code>forward(im, t)</code> that maps a
        clean 64×64 Campanile image to a noisy version at timestep <code>t</code>. At each timestep, the
        function adds Gaussian noise with variance prescribed by the fixed noise schedule, so that larger
        values of <code>t</code> correspond to stronger corruption. This defines a monotonic path from the
        data distribution toward an isotropic Gaussian, which is the starting point for the reverse
        denoising process in later parts of the project.
      </p>
    </div>


    <div class="iteration-belt__header">
      <!-- <h3 class="iteration-belt__title">Renders</h3> -->
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">Pause</button>
    </div>

    <div class="iteration-belt__viewport">
      <h4 class="iteration-belt__row-title">Timesteps</h4>
      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/forward_process/noisy_t_0.png"
            alt="Data visualization at iteration 0">
          <figcaption class="iteration-belt__caption">t = 0</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/forward_process/noisy_t_250.png"
            alt="Data visualization at iteration 250">
          <figcaption class="iteration-belt__caption">t = 250</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/forward_process/noisy_t_500.png"
            alt="Data visualization at iteration 500">
          <figcaption class="iteration-belt__caption">t = 500</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/forward_process/noisy_t_750.png"
            alt="Data visualization at iteration 750">
          <figcaption class="iteration-belt__caption">t = 750</figcaption>
        </figure>
      </div>
    </div>


    <div class="iteration-belt__outro">

      <p>
        To visualize this behavior, the Campanile is shown at several representative timesteps:
        <code>t = 0</code>, <code>250</code>, <code>400</code>, <code>500</code>, and <code>750</code>.
        At <code>t = 0</code>, the image is the original, noise-free photograph. By <code>t = 250</code>,
        moderate noise is visible, but the tower, trees, and sky remain clearly recognizable. At
        <code>t = 500</code>, the injected noise dominates fine detail: edges
        blur, colors fluctuate more erratically, and the Campanile is reduced to a faint silhouette. By
        <code>t = 750</code>, the sample is visually indistinguishable from pure noise, with only faint
        trace of the original structure.
      </p>
    </div>


  </section>

  <!-- ============================================ 1.2 Classical Denoising =============================================== -->
  <section class="content gold-glass iteration-belt">
    <h2 class="section-heading">1.2 Classical Denoising</h2>
    <div class="iteration-belt__intro">
      <p>
        Classical denoising here is instantiated by a simple Gaussian blur applied directly to the noisy
        Campanile images from the forward process. For each noise level (t = 0, 250, 500, 750), the noisy
        image is filtered with a fixed-radius Gaussian kernel, which suppresses high-frequency variations
        but does not use any knowledge of the underlying data distribution. This produces a purely
        low-pass baseline that can be compared against the diffusion-based denoisers in later parts of the
        project.
      </p>

    </div>


    <div class="iteration-belt__header">
      <!-- <h3 class="iteration-belt__title">Renders</h3> -->
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">Pause</button>
    </div>


    <div class="iteration-belt__viewport">
      <h4 class="iteration-belt__row-title">Timesteps</h4>
      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/classical_denoising/noisy_t_0.png" alt="Noisy image t = 0">
          <figcaption class="iteration-belt__caption">t = 0</figcaption>
        </figure>


        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/classical_denoising/noisy_t_250.png" alt="Noisy image t = 250">
          <figcaption class="iteration-belt__caption">t = 250</figcaption>
        </figure>


        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/classical_denoising/noisy_t_500.png" alt="Noisy image t = 500">
          <figcaption class="iteration-belt__caption">t = 500</figcaption>
        </figure>


        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/classical_denoising/noisy_t_750.png" alt="Noisy image t = 750">
          <figcaption class="iteration-belt__caption">t = 750</figcaption>
        </figure>


      </div>
    </div>


    <div class="iteration-belt__outro">
      <p>
        The sequence of results highlights both the strengths and limitations of this approach. At low
        noise levels (t = 0 and t = 250), Gaussian denoising successfully reduces speckle in the sky and
        façade while preserving the overall silhouette of the tower and trees, although fine edges become
        noticeably softened. At higher noise levels (t = 500 and especially t = 750), the blur removes some
        of the colorful grain but also smears out important structure, leaving a hazy, almost watercolor
        version of the scene in which architectural details and tree textures are largely lost. This
        behavior illustrates why classical smoothing is a useful but ultimately insufficient baseline:
        it can attenuate noise, but without a learned prior it cannot reconstruct missing high-frequency
        detail once the forward process has heavily corrupted the image.
      </p>
    </div>


  </section>

  <!-- ============================================ 1.3 One-Step Denoising =============================================== -->
  <section class="content gold-glass iteration-belt iteration-belt--two-rows"
    aria-label="One-step denoising comparison">
    <h2 class="section-heading">1.3 One-Step Denoising</h2>

    <div class="iteration-belt__intro">
      <p>
        One-step denoising evaluates how much structure can be recovered from a noisy observation using
        a single reverse diffusion step. For each timestep <em>t</em> in {250, 500, 750}, the forward
        process is first used to corrupt the clean 64×64 Campanile image and obtain a noisy input. This
        noisy image is then passed through <code>stage_1.unet</code> to estimate the noise component,
        and a one-step reconstruction is formed by subtracting the predicted noise from the noisy input.
        The resulting estimate can be compared directly against the original image and the noisy
        observation to assess how well a single denoising step in the learned model inverts the forward
        process at different noise levels.
      </p>
    </div>

    <div class="iteration-belt__header">
      <!-- optional small title; can be commented out if you prefer only the Pause button -->
      <!-- <h3 class="iteration-belt__title">One-Step Denoising (Noisy vs Estimation)</h3> -->
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">
        Pause
      </button>
    </div>

    <!-- Row 1: noisy inputs -->
    <div class="iteration-belt__row">
      <h3 class="iteration-belt__row-title">Noisy input</h3>
      <div class="iteration-belt__viewport">
        <div class="iteration-belt__track">
          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/one_step/noisy_t_0.png" alt="Noisy image at t = 0">
            <figcaption class="iteration-belt__caption">t = 0</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/one_step/noisy_t_250.png" alt="Noisy image at t = 250">
            <figcaption class="iteration-belt__caption">t = 250</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/one_step/noisy_t_500.png" alt="Noisy image at t = 500">
            <figcaption class="iteration-belt__caption">t = 500</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/one_step/noisy_t_750.png" alt="Noisy image at t = 750">
            <figcaption class="iteration-belt__caption">t = 750</figcaption>
          </figure>
        </div>
      </div>
    </div>

    <!-- Row 2: one-step denoised estimates -->
    <div class="iteration-belt__row">
      <h3 class="iteration-belt__row-title">One-step denoised estimate</h3>
      <div class="iteration-belt__viewport">
        <div class="iteration-belt__track">
          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/one_step/denoised_t_0.png"
              alt="One-step denoised image at t = 0">
            <figcaption class="iteration-belt__caption">t = 0</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/one_step/denoised_t_250.png"
              alt="One-step denoised image at t = 250">
            <figcaption class="iteration-belt__caption">t = 250</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/one_step/denoised_t_500.png"
              alt="One-step denoised image at t = 500">
            <figcaption class="iteration-belt__caption">t = 500</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/one_step/denoised_t_750.png"
              alt="One-step denoised image at t = 750">
            <figcaption class="iteration-belt__caption">t = 750</figcaption>
          </figure>
        </div>
      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        As the noise level increases, the one-step reconstructions retain the coarse outline of the
        Campanile but lose fine detail and contrast, highlighting the limitations of a single reverse
        step and motivating the iterative denoising scheme developed in the next section.
      </p>
    </div>
  </section>


  <!-- ============================================ 1.4 Iterative Denoising =============================================== -->
  <section class="content gold-glass iteration-belt iteration-belt--two-rows"
    aria-label="Iterative denoising comparison">
    <h2 class="section-heading">1.4 Iterative Denoising</h2>

    <div class="iteration-belt__intro">
      <p>
        Iterative denoising generalizes the one-step procedure by applying the learned noise predictor
        across a full schedule of reverse diffusion steps. Starting from a heavily noised Campanile
        image at an initial index <code>i_start = 10</code>, a strided list of timesteps is constructed
        (from 990 down to 0 in steps of 30), and at each timestep the UNet estimates the noise and
        performs a small update toward the clean image. This produces a sequence of intermediate
        reconstructions that progressively recover large-scale structure and fine detail, rather than
        attempting to undo all of the corruption in a single jump.
      </p>
    </div>

    <div class="iteration-belt__header">
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">
        Pause
      </button>
    </div>

    <!-- Row 1: iterative denoising trajectory (GIFs every few steps) -->
    <div class="iteration-belt__row">
      <h3 class="iteration-belt__row-title">Iterative denoising trajectory</h3>
      <div class="iteration-belt__viewport">
        <div class="iteration-belt__track">

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/iterative_denoising/i_start1.gif"
              alt="Iterative denoising from early timestep">
            <figcaption class="iteration-belt__caption">early steps</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/iterative_denoising/i_start3.gif"
              alt="Iterative denoising after several updates">
            <figcaption class="iteration-belt__caption">midway (1)</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/iterative_denoising/i_start5.gif"
              alt="Iterative denoising after more updates">
            <figcaption class="iteration-belt__caption">midway (2)</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/iterative_denoising/i_start7.gif"
              alt="Iterative denoising near the end of the schedule">
            <figcaption class="iteration-belt__caption">late steps</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/iterative_denoising/i_start10.gif"
              alt="Iterative denoising with more aggressive schedule">
            <figcaption class="iteration-belt__caption">aggressive schedule</figcaption>
          </figure>

        </div>
      </div>
    </div>

    <!-- Row 2: final reconstructions compared -->
    <div class="iteration-belt__row">
      <h3 class="iteration-belt__row-title">Final reconstructions</h3>
      <div class="iteration-belt__viewport">
        <div class="iteration-belt__track">

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/iterative_denoising/orig.png" alt="Original image">
            <figcaption class="iteration-belt__caption">Original Campanile</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/iterative_denoising/i_start20.png"
              alt="Iterative denoised Campanile">
            <figcaption class="iteration-belt__caption">Iterative denoising</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/one_step/denoised_t_500.png"
              alt="One-step denoised Campanile">
            <figcaption class="iteration-belt__caption">One-step denoising</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/classical_denoising/gauss_t_500.png"
              alt="Gaussian blurred Campanile">
            <figcaption class="iteration-belt__caption">Gaussian baseline</figcaption>
          </figure>

        </div>
      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        The GIFs in the first row show the noisy Campanile gradually emerging from near-pure noise:
        early frames retain little structure, mid-sequence frames recover the tower silhouette and sky
        gradient, and late frames sharpen edges and contrast. The second row compares the final
        iterative reconstruction against the original image, the one-step estimate, and the classical
        Gaussian baseline. Iterative denoising produces the closest match to the original: the tower
        and trees are well defined and textures are plausibly restored. The one-step estimate recovers
        only coarse shape and remains noticeably washed out, while the Gaussian result is over-smoothed
        and lacks architectural detail. This comparison demonstrates that repeatedly applying the
        learned reverse step along a carefully chosen schedule is essential for high-quality
        reconstructions in diffusion models.
      </p>
    </div>
  </section>


  <!-- ============================================ 1.5 Diffusion Model Sampling =============================================== -->
  <section class="content gold-glass iteration-belt" aria-label="Diffusion model sampling">
    <h2 class="section-heading">1.5 Diffusion Model Sampling</h2>

    <div class="iteration-belt__intro">
      <p>
        Diffusion model sampling reverses the forward corruption process by starting from pure Gaussian
        noise and applying the learned denoising model along the full schedule of timesteps. For this
        part, the stage&nbsp;1 sampler is run five times with different random seeds
        (32168, 100570, 91798, 1215, 122607). Each run draws an independent noise image at the final
        timestep and then traverses the entire reverse chain, producing one synthesized 64×64 image per
        seed. Model weights, scheduler settings, and all hyperparameters are held fixed; only the
        initial noise realization changes. This configuration directly realizes the marginal
        distribution learned by the model and satisfies the requirement to “show 5 sampled images.”
      </p>
    </div>

    <div class="iteration-belt__header">
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">
        Pause
      </button>
    </div>

    <div class="iteration-belt__row">
      <h3 class="iteration-belt__row-title">Samples at timestep t = 0</h3>
      <div class="iteration-belt__viewport">
        <div class="iteration-belt__track">

          <!-- 5 sampled images -->
          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/diffusion_model_sampling/sample_32168.png"
              alt="Sampled landscape with flowers and a pier at sunrise">
            <figcaption class="iteration-belt__caption">
              Seed 32168 – lakeside sunrise with foreground flowers
            </figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/diffusion_model_sampling/sample_100570.png"
              alt="Grid of architectural and landscape thumbnails">
            <figcaption class="iteration-belt__caption">
              Seed 100570 – collage of coastal and architectural scenes
            </figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/diffusion_model_sampling/sample_91798.png"
              alt="Portrait of a person looking up in a storm of particles">
            <figcaption class="iteration-belt__caption">
              Seed 91798 – portrait in a field of glowing particles
            </figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/diffusion_model_sampling/sample_1215.png"
              alt="Monochrome cityscape partially obscured by smoke">
            <figcaption class="iteration-belt__caption">
              Seed 1215 – atmospheric monochrome cityscape
            </figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/diffusion_model_sampling/sample_122607.png"
              alt="Figure walking along pastel rock formations at sunset">
            <figcaption class="iteration-belt__caption">
              Seed 122607 – figure in pastel desert landscape
            </figcaption>
          </figure>

        </div>
      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        The five samples exhibit diverse scene types and compositions—ranging from a flower-framed
        lakeside at sunrise to a portrait immersed in particles and a pastel desert landscape—despite
        being generated by the same model and schedule. This diversity arises solely from the different
        random seeds, which determine the initial noise pattern at the start of the reverse process.
        At the same time, all images share consistent photographic qualities such as coherent lighting,
        natural textures, and plausible perspective, indicating that the model has learned a strong
        prior over natural images rather than memorizing individual examples. These observations
        confirm that the sampler is correctly traversing the learned image distribution.
      </p>
    </div>
  </section>


  <!-- ============================================ 1.6 Classifier-Free Guidance (CFG) =============================================== -->
  <section class="content gold-glass iteration-belt" aria-label="CFG diffusion sampling">
    <h2 class="section-heading">1.6 Classifier-Free Guidance (CFG)</h2>

    <div class="iteration-belt__intro">
      <p>
        Classifier-free guidance modifies the sampling process by interpolating between unconditional
        and text-conditioned predictions from the UNet. At each reverse step, the model is run twice:
        once with the prompt dropped (unconditional) and once with the prompt “a high quality photo.”
        The final noise estimate is obtained by adding a scaled difference between these two outputs,
        controlled by a guidance scale <em>w</em>. Small values of <em>w</em> keep the samples close to
        the unconditional image distribution and emphasize diversity, whereas large values force the
        generation to adhere more strongly to the prompt, at the risk of oversharpening or introducing
        artifacts. In this section, five images are generated with a fixed random seed and guidance
        scales {2, 4, 5, 6, 8}, satisfying the requirement to show five CFG samples for the same text
        description.
      </p>
    </div>

    <div class="iteration-belt__header">
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">
        Pause
      </button>
    </div>

    <div class="iteration-belt__row">
      <h3 class="iteration-belt__row-title">Samples with varying CFG scales</h3>
      <div class="iteration-belt__viewport">
        <div class="iteration-belt__track">

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/cfg/s2.png" alt="Soft portrait sampled with low CFG scale">
            <figcaption class="iteration-belt__caption">
              CFG scale 2 – soft, low-contrast portrait
            </figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/cfg/s4.png"
              alt="Monochrome portrait with stronger definition">
            <figcaption class="iteration-belt__caption">
              CFG scale 4 – more defined monochrome portrait
            </figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/cfg/s5.png" alt="Close-up portrait with vivid colors">
            <figcaption class="iteration-belt__caption">
              CFG scale 5 – vivid close-up portrait
            </figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/cfg/s6.png" alt="Landscape at dusk with strong reflections">
            <figcaption class="iteration-belt__caption">
              CFG scale 6 – high-contrast landscape at dusk
            </figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/cfg/s8.png" alt="Bright portrait in a green field">
            <figcaption class="iteration-belt__caption">
              CFG scale 8 – very sharp, strongly conditioned portrait
            </figcaption>
          </figure>

        </div>
      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        As the CFG scale increases, the samples move from soft, low-contrast renderings toward images
        with sharper edges, stronger color saturation, and more “on-prompt” structure. At low guidance
        (e.g., scale 2), the model behaves similarly to the unconditional sampler: compositions are
        plausible but loosely tied to the prompt. Intermediate scales (around 4–6) balance fidelity and
        realism, producing images that read clearly as high-quality photographs without obvious
        artifacts. At the highest scale (8), the model adheres very strongly to the prompt, which
        enhances detail but can also exaggerate contrast and make textures look slightly artificial.
        This progression illustrates the core trade-off of classifier-free guidance: stronger guidance
        improves prompt alignment but gradually reduces diversity and can push samples away from the
        natural image manifold if set too high.
      </p>
    </div>
  </section>


  <!-- ============================================ 1.7 Image-to-image Translation (SDEdit) ===================================== -->
  <section class="content gold-glass iteration-belt iteration-belt--two-rows"
    aria-label="Image-to-image translation with SDEdit">
    <h2 class="section-heading">1.7 Image-to-image Translation (SDEdit)</h2>

    <div class="iteration-belt__intro">
      <p>
        Image-to-image translation in this part is implemented using the SDEdit procedure. Instead of
        sampling from pure noise, the algorithm begins from a real 64×64 photograph of the Campanile,
        runs the forward diffusion process up to a chosen noise index <em>i_start</em>, and then applies
        the usual reverse sampler conditioned on the prompt “a high quality photo.” For small values of
        <em>i_start</em>, the starting point is only mildly corrupted and the sampler behaves like a
        noise-removal-and-retouch operator; larger values push the image closer to pure noise before
        denoising, giving the model more freedom to reinterpret the scene while still being guided by
        the original structure.
      </p>
    </div>

    <div class="iteration-belt__header">
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">
        Pause
      </button>
    </div>

    <!-- Row 1: Campanile SDEdit -->
    <div class="iteration-belt__row">
      <h3 class="iteration-belt__row-title">Campanile: noise level sweep</h3>
      <div class="iteration-belt__viewport">
        <div class="iteration-belt__track">

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/i_start1.gif" alt="Campanile SDEdit i_start = 1">
            <figcaption class="iteration-belt__caption">noise level 1</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/i_start3.gif" alt="Campanile SDEdit i_start = 3">
            <figcaption class="iteration-belt__caption">noise level 3</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/i_start5.gif" alt="Campanile SDEdit i_start = 5">
            <figcaption class="iteration-belt__caption">noise level 5</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/i_start7.gif" alt="Campanile SDEdit i_start = 7">
            <figcaption class="iteration-belt__caption">noise level 7</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/i_start10.gif" alt="Campanile SDEdit i_start = 10">
            <figcaption class="iteration-belt__caption">noise level 10</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/i_start20.gif" alt="Campanile SDEdit i_start = 20">
            <figcaption class="iteration-belt__caption">noise level 20</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/orig.png" alt="Original Campanile">
            <figcaption class="iteration-belt__caption">Original input</figcaption>
          </figure>

        </div>
      </div>
    </div>

    <!-- Row 2: Glass Art example -->
    <div class="iteration-belt__row">
      <h3 class="iteration-belt__row-title">Glass Art: Chihuly-style sculpture</h3>
      <div class="iteration-belt__viewport">
        <div class="iteration-belt__track">

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/a/i_start1.gif"
              alt="Glass sculpture SDEdit i_start = 1">
            <figcaption class="iteration-belt__caption">noise level 1</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/a/i_start3.gif"
              alt="Glass sculpture SDEdit i_start = 3">
            <figcaption class="iteration-belt__caption">noise level 3</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/a/i_start5.gif"
              alt="Glass sculpture SDEdit i_start = 5">
            <figcaption class="iteration-belt__caption">noise level 5</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/a/i_start7.gif"
              alt="Glass sculpture SDEdit i_start = 7">
            <figcaption class="iteration-belt__caption">noise level 7</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/a/i_start10.gif"
              alt="Glass sculpture SDEdit i_start = 10">
            <figcaption class="iteration-belt__caption">noise level 10</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/a/i_start20.gif"
              alt="Glass sculpture SDEdit i_start = 20">
            <figcaption class="iteration-belt__caption">noise level 20</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/a/orig.png" alt="Original glass sculpture">
            <figcaption class="iteration-belt__caption">Original input</figcaption>
          </figure>

        </div>
      </div>
    </div>

    <!-- Row 3: Space Needle example -->
    <div class="iteration-belt__row">
      <h3 class="iteration-belt__row-title">Space Needle: cityscape reinterpretation</h3>
      <div class="iteration-belt__viewport">
        <div class="iteration-belt__track">

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/b/i_start1.gif" alt="Space Needle SDEdit i_start = 1">
            <figcaption class="iteration-belt__caption">noise level 1</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/b/i_start3.gif" alt="Space Needle SDEdit i_start = 3">
            <figcaption class="iteration-belt__caption">noise level 3</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/b/i_start5.gif" alt="Space Needle SDEdit i_start = 5">
            <figcaption class="iteration-belt__caption">noise level 5</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/b/i_start7.gif" alt="Space Needle SDEdit i_start = 7">
            <figcaption class="iteration-belt__caption">noise level 7</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/b/i_start10.gif"
              alt="Space Needle SDEdit i_start = 10">
            <figcaption class="iteration-belt__caption">noise level 10</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/b/i_start20.gif"
              alt="Space Needle SDEdit i_start = 20">
            <figcaption class="iteration-belt__caption">noise level 20</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/SDEdit/b/orig.png" alt="Original Space Needle photograph">
            <figcaption class="iteration-belt__caption">Original input</figcaption>
          </figure>

        </div>
      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        Across all three examples, the SDEdit trajectory shows a consistent pattern. For the Campanile,
        low noise levels preserve the tower and skyline almost exactly, while higher levels introduce
        softer clouds, altered color grading, and occasionally new structural details. The glass
        sculpture begins as a dense cluster of orange spikes and, under increasing noise, is redrawn
        as more diffuse, branching forms that still respect the radial “explosion” motif but explore
        new lighting and background colors. The Space Needle sequence similarly moves from a faithful
        reconstruction of the original skyline to more stylized versions where the tower, clouds, and
        city lights are exaggerated or partially reimagined. Together, these rows demonstrate that
        SDEdit can either gently refine an existing photograph or drive it toward a more creative,
        prompt-driven reinterpretation, depending on how far along the forward diffusion trajectory
        the sampler chooses to re-enter.
      </p>
    </div>
  </section>

  <!-- ============================================ 1.7.1 Editing Web and Hand-Drawn Images ===================================== -->
  <section class="content gold-glass iteration-belt iteration-belt--two-rows"
    aria-label="Editing web and hand-drawn images with SDEdit">
    <h2 class="section-heading">1.7.1 Editing Web and Hand-Drawn Images</h2>

    <div class="iteration-belt__intro">
      <p>
        This section tests SDEdit on images that do not come from the Campanile dataset. One image is
        sourced from the web (a high-contrast photograph of the Crab Nebula), and two are hand-drawn
        doodles (“The Blue Bandit” and “The Little Prince”). For each input, the forward process is
        run to noise levels <em>i_start</em> ∈ {1, 3, 5, 7, 10, 20}, and the reverse sampler is then
        applied under the prompt “a high quality photo.” At low noise levels, the procedure behaves
        like a local enhancement operator that cleans and stylizes the input while preserving its
        structure; at high noise levels, the model is forced to hallucinate more content from its
        learned prior while still being anchored by the original image. These sequences satisfy the
        deliverable requirement of editing one web image and two hand-drawn images across the specified
        noise levels.
      </p>
    </div>

    <div class="iteration-belt__header">
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">
        Pause
      </button>
    </div>

    <!-- Row 1: Web image (Crab Nebula) -->
    <div class="iteration-belt__row">
      <h3 class="iteration-belt__row-title">Web image: The Crab Nebula</h3>
      <div class="iteration-belt__viewport">
        <div class="iteration-belt__track">

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/web/orig.png"
              alt="Rescaled 64×64 Crab Nebula input image">
            <figcaption class="iteration-belt__caption">Rescaled input (64×64)</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/web/i_start1.gif"
              alt="Crab Nebula SDEdit i_start = 1">
            <figcaption class="iteration-belt__caption">noise level 1</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/web/i_start3.gif"
              alt="Crab Nebula SDEdit i_start = 3">
            <figcaption class="iteration-belt__caption">noise level 3</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/web/i_start5.gif"
              alt="Crab Nebula SDEdit i_start = 5">
            <figcaption class="iteration-belt__caption">noise level 5</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/web/i_start7.gif"
              alt="Crab Nebula SDEdit i_start = 7">
            <figcaption class="iteration-belt__caption">noise level 7</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/web/i_start10.gif"
              alt="Crab Nebula SDEdit i_start = 10">
            <figcaption class="iteration-belt__caption">noise level 10</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/web/i_start20.gif"
              alt="Crab Nebula SDEdit i_start = 20">
            <figcaption class="iteration-belt__caption">noise level 20</figcaption>
          </figure>

        </div>
      </div>
    </div>

    <!-- Row 2: Doodle 1 – The Blue Bandit -->
    <div class="iteration-belt__row">
      <h3 class="iteration-belt__row-title">Doodle 1: The Blue Bandit</h3>
      <div class="iteration-belt__viewport">
        <div class="iteration-belt__track">

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/doodle1/orig.png" alt="Hand-drawn Blue Bandit input">
            <figcaption class="iteration-belt__caption">Rescaled input (64×64)</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/doodle1/i_start1.gif"
              alt="Blue Bandit SDEdit i_start = 1">
            <figcaption class="iteration-belt__caption">noise level 1</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/doodle1/i_start3.gif"
              alt="Blue Bandit SDEdit i_start = 3">
            <figcaption class="iteration-belt__caption">noise level 3</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/doodle1/i_start5.gif"
              alt="Blue Bandit SDEdit i_start = 5">
            <figcaption class="iteration-belt__caption">noise level 5</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/doodle1/i_start7.gif"
              alt="Blue Bandit SDEdit i_start = 7">
            <figcaption class="iteration-belt__caption">noise level 7</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/doodle1/i_start10.gif"
              alt="Blue Bandit SDEdit i_start = 10">
            <figcaption class="iteration-belt__caption">noise level 10</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/doodle1/i_start20.gif"
              alt="Blue Bandit SDEdit i_start = 20">
            <figcaption class="iteration-belt__caption">noise level 20</figcaption>
          </figure>

        </div>
      </div>
    </div>

    <!-- Row 3: Doodle 2 – The Little Prince -->
    <div class="iteration-belt__row">
      <h3 class="iteration-belt__row-title">Doodle 2: The Little Prince</h3>
      <div class="iteration-belt__viewport">
        <div class="iteration-belt__track">

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/doodle2/orig.png"
              alt="Hand-drawn Little Prince input">
            <figcaption class="iteration-belt__caption">Rescaled input (64×64)</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/doodle2/i_start1.gif"
              alt="Little Prince SDEdit i_start = 1">
            <figcaption class="iteration-belt__caption">noise level 1</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/doodle2/i_start3.gif"
              alt="Little Prince SDEdit i_start = 3">
            <figcaption class="iteration-belt__caption">noise level 3</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/doodle2/i_start5.gif"
              alt="Little Prince SDEdit i_start = 5">
            <figcaption class="iteration-belt__caption">noise level 5</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/doodle2/i_start7.gif"
              alt="Little Prince SDEdit i_start = 7">
            <figcaption class="iteration-belt__caption">noise level 7</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/doodle2/i_start10.gif"
              alt="Little Prince SDEdit i_start = 10">
            <figcaption class="iteration-belt__caption">noise level 10</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/hand_web/doodle2/i_start20.gif"
              alt="Little Prince SDEdit i_start = 20">
            <figcaption class="iteration-belt__caption">noise level 20</figcaption>
          </figure>

        </div>
      </div>
    </div>

    <div class="iteration-belt__outro">
      <p>
        The Crab Nebula sequence shows that, even for a complex astronomical scene, low noise levels
        allow the sampler to clean and sharpen filaments while preserving the overall color palette and
        radial structure. At higher noise, the model still reconstructs a nebula-like cloud but freely
        adjusts fine filament geometry and local lighting. For the hand-drawn doodles, the behavior is
        even more revealing: starting from simple line art, low <em>i_start</em> values yield
        photo-like renderings that closely follow the original silhouettes (for example, the masked
        face of the Blue Bandit or the child and planet in The Little Prince), while higher values
        gradually replace sketchy strokes with richer shading, textured backgrounds, and new
        high-frequency detail. Across all three inputs, the SDEdit edits confirm that the diffusion
        model can project both photographic and non-photographic images onto its learned natural image
        manifold, controlling the strength of the transformation through a single noise-level
        parameter.
      </p>
    </div>
  </section>



  <!-- ==================================================== 1.7.2 Inpainting ====================================================== -->
  <section class="content gold-glass writing-section">
    <h2 class="section-heading">1.7.2 Inpainting</h2>
    <p>
      1 image from the web of your choice, edited using the above method for noise levels [1, 3, 5, 7, 10, 20] (and
      whatever additional noise levels you want)
      2 hand drawn images, edited using the above method for noise levels [1, 3, 5, 7, 10, 20] (and whatever additional
      noise levels you want)
    </p>
  </section>

  <section class="content gold-glass iteration-belt">
    <div class="iteration-belt__header">
      <h2 class="iteration-belt__title">Inpainting The Campanile</h2>
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">
        Pause
      </button>
    </div>

    <div class="iteration-belt__viewport">
      <h3 class="iteration-belt__row-title">Timesteps </h3>
      <div class="iteration-belt__track">

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/inpainting/campanile/orig.png">
          <figcaption class="iteration-belt__caption">Input Image </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/inpainting/campanile/camp_double_mask.png">
          <figcaption class="iteration-belt__caption">Mask</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/inpainting/campanile/i_start1.gif">
          <figcaption class="iteration-belt__caption">start index = 1</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/inpainting/campanile/i_start2.gif">
          <figcaption class="iteration-belt__caption">start index = 2</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/inpainting/campanile/i_start3.gif">
          <figcaption class="iteration-belt__caption">start index = 3</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/inpainting/campanile/i_start4.gif">
          <figcaption class="iteration-belt__caption">start index = 4</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/inpainting/campanile/i_start5.gif">
          <figcaption class="iteration-belt__caption">start index = 5</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/inpainting/campanile/i_start7.gif">
          <figcaption class="iteration-belt__caption">start index = 7</figcaption>
        </figure>
      </div>
    </div>
  </section>

  <!-- ==================================================== 1.7.3 Text-Conditional Image-to-image Translation ====================================================== -->
  <section class="content gold-glass writing-section">
    <h2 class="section-heading">Text-Conditional Image-to-image Translation</h2>
    <p>
      Now, we will do the same thing as SDEdit, but guide the projection with a text prompt. This is no longer pure
      "projection to the natural image manifold" but also adds control using language. This is simply a matter of
      changing the prompt from "a high quality photo" to any of your prompt!
      Deliverables

      Edits of the Campanile, using the given prompt at noise levels [1, 3, 5, 7, 10, 20]
      Edits of 2 of your own test images, using the same procedure

      Hints

      The images should gradually look more like original image, but also look like the text prompt.

    </p>
  </section>

  <section class="content gold-glass iteration-belt">
    <div class="iteration-belt__header">
      <!-- <h2 class="iteration-belt__title">Inpainting The Campanile</h2> -->
      <h2 class="iteration-belt__title">1.7.3 Prompt Guided Diffusion Models</h2>
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">
        Pause
      </button>
    </div>

    <div class="iteration-belt__viewport">
      <h3 class="iteration-belt__row-title">Prompt: "a rocketship launching into space"</h3>
      <div class="iteration-belt__track">

        <!-- <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/inpainting/campanile/orig.png">
          <figcaption class="iteration-belt__caption">Input Image </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/inpainting/campanile/camp_double_mask.png">
          <figcaption class="iteration-belt__caption">Mask</figcaption>
        </figure> -->

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/txtCond_img2img/i_start1.gif">
          <figcaption class="iteration-belt__caption">noise level 1</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/txtCond_img2img/i_start3.gif">
          <figcaption class="iteration-belt__caption">noise level 3</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/txtCond_img2img/i_start5.gif">
          <figcaption class="iteration-belt__caption">noise level 5</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/txtCond_img2img/i_start7.gif">
          <figcaption class="iteration-belt__caption">noise level 7</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/txtCond_img2img/i_start10.gif">
          <figcaption class="iteration-belt__caption">noise level 10</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/txtCond_img2img/i_start20.gif">
          <figcaption class="iteration-belt__caption">noise level 20</figcaption>
        </figure>
      </div>
    </div>
  </section>

  <!-------------------------------------------------------------1.7.3 image A ------------------------------------------>
  <section class="content gold-glass writing-section">
    <h2 class="section-heading">Text-Conditional Image-to-image Translation</h2>
    <p>
      1 image from the web of your choice, edited using the above method for noise levels [1, 3, 5, 7, 10, 20] (and
      whatever additional noise levels you want)
      2 hand drawn images, edited using the above method for noise levels [1, 3, 5, 7, 10, 20] (and whatever additional
      noise levels you want)
    </p>
  </section>

  <section class="content gold-glass iteration-belt iteration-belt--two-rows" delta-speed>
    <div class="iteration-belt__header">
      <h2 class="iteration-belt__title">Prompt Guided Diffusion Models</h2>
      <button type="button" class="iteration-belt__toggle" aria-pressed="false">
        Pause
      </button>
    </div>

    <!-- Row 2: iterative denoise inputs -->
    <div class="iteration-belt__row">
      <h3 class="iteration-belt__row-title">Prompt: "a Renaissance painting of a knight and a dragon"</h3>
      <div class="iteration-belt__viewport">
        <div class="iteration-belt__track">


          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/a/i_start1.gif" alt="id start = 1">
            <figcaption class="iteration-belt__caption">noise level 1</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/a/i_start3.gif" alt="id_start = 3">
            <figcaption class="iteration-belt__caption">noise level 3</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/a/i_start5.gif" alt="id_start = 5">
            <figcaption class="iteration-belt__caption">noise level 5</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/a/i_start7.gif" alt="id_start = 7">
            <figcaption class="iteration-belt__caption">noise level 7</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/a/i_start10.gif" alt="id start = 10">
            <figcaption class="iteration-belt__caption">noise level 10</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/a/i_start17.gif" alt="id start = 10">
            <figcaption class="iteration-belt__caption">noise level 17</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/a/i_start20.gif" alt="id start = start">
            <figcaption class="iteration-belt__caption">noise level 20</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/a/orig.png" alt="id start = start">
            <figcaption class="iteration-belt__caption">Original</figcaption>
          </figure>
        </div>
      </div>
    </div>

    <div class="iteration-belt__row">
      <h3 class="iteration-belt__row-title">Prompt: "'a Renaissance painting of a celestial choir of angels"</h3>
      <div class="iteration-belt__viewport">
        <div class="iteration-belt__track">

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/b/i_start1.gif" alt="id start = 1">
            <figcaption class="iteration-belt__caption">noise level 1</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/b/i_start3.gif" alt="id_start = 3">
            <figcaption class="iteration-belt__caption">noise level 3</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/b/i_start5.gif" alt="id_start = 5">
            <figcaption class="iteration-belt__caption">noise level 5</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/b/i_start7.gif" alt="id_start = 7">
            <figcaption class="iteration-belt__caption">noise level 7</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/b/i_start10.gif" alt="id start = 10">
            <figcaption class="iteration-belt__caption">noise level 10</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/b/i_start17.gif" alt="id start = 17">
            <figcaption class="iteration-belt__caption">noise level 17</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/b/i_start20.gif" alt="id start = 20">
            <figcaption class="iteration-belt__caption">noise level 20</figcaption>
          </figure>

          <figure class="iteration-belt__item">
            <img class="iteration-belt__image" src="media/txtCond_img2img/b/orig.png" alt="id start = original">
            <figcaption class="iteration-belt__caption">Original</figcaption>
          </figure>
        </div>
      </div>
    </div>


  </section>

  <!-- ==================================================== 1.8 Visual Anagrams ====================================================== -->

  <section class="content gold-glass writing-section">
    <h2 class="section-heading">Visual Anagrams</h2>
    <p>
      In this part, we are finally ready to implement Visual Anagrams and create optical illusions with diffusion
      models. In this part, we will create an image that looks like "an oil painting of people around a campfire", but
      when flipped upside down will reveal "an oil painting of an old man".

      To do this, we will denoise an image
      at step normally with the prompt , to obtain noise estimate . But at the same time, we will flip upside down, and
      denoise with the prompt , to get noise estimate . We can flip

      back, and average the two noise estimates. We can then perform a reverse/denoising diffusion step with the
      averaged noise estimate.

      The full algorithm will be:

      where UNet is the diffusion model UNet from before, is a function that flips the image, and and are two different
      text prompt embeddings. And our final noise estimate is

      . Please implement the above algorithm and show example of an illusion.
      Deliverables

      Correctly implemented visual_anagrams function
      2 illusions of your choice that change appearance when you flip it upside down (feel free to take inspirations
      from this page).

      Hints

      You may have to run multiple times to get a really good result for the same reasons as above.

    </p>
  </section>

  <section class="content gold-glass iteration-belt">
    <div class="iteration-belt__header">
      <h2 class="iteration-belt__title">Visual Anagrams</h2>

      <button type="button" class="iteration-belt__toggle" aria-pressed="false">
        Pause
      </button>
    </div>

    <div class="iteration-belt__viewport">
      <h3 class="iteration-belt__row-title">Various Prompts</h3>

      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/visual_anagrams/anagram_flipped.png">
          <figcaption class="iteration-belt__caption">an oil painting of an old man</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/visual_anagrams/anagram_upright.png">
          <figcaption class="iteration-belt__caption">an oil painting of people around a campfire</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/visual_anagrams/a/anagram_upright.png">
          <figcaption class="iteration-belt__caption">a baroque painting of Cleopatra as a newborn princess in a tall
            palace hall</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/visual_anagrams/a/anagram_flipped.png">
          <figcaption class="iteration-belt__caption">a baroque painting of Cleopatra as a dying queen in the same tall
            palace hall</figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/visual_anagrams/b/anagram_flipped.png">
          <figcaption class="iteration-belt__caption">a romanticism painting of the storming of the Bastille
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/visual_anagrams/b/anagram_upright.png">
          <figcaption class="iteration-belt__caption"> a neoclassical painting of the signing of the United States
            Declaration of Independence</figcaption>
        </figure>
      </div>
    </div>
  </section>
  <!-- ==================================================== 1.9 Hybrid Images ====================================================== -->

  <section class="content gold-glass writing-section">
    <h2 class="section-heading">Visual Anagrams</h2>
    <p>
      Deliverables

      Correctly implemented make_hybrids function
      2 hybrid images of your choosing (feel free to take inspirations from this page).

      Hints

      use torchvision.transforms.functional.gaussian_blur
      You may have to run multiple times to get a really good result for the same reasons as above
    </p>
  </section>

  <section class="content gold-glass iteration-belt">
    <div class="iteration-belt__header">
      <h2 class="iteration-belt__title">Hybrid Images</h2>

      <button type="button" class="iteration-belt__toggle" aria-pressed="false">
        Pause
      </button>
    </div>

    <div class="iteration-belt__viewport">
      <h3 class="iteration-belt__row-title">Various Prompts</h3>

      <div class="iteration-belt__track">
        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/hybrid/a/hybrid_steps.gif">
          <figcaption class="iteration-belt__caption">"a lithograph of a skull" / "a lithograph of a waterfall"
          </figcaption>
        </figure>

        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/hybrid/b/hybrid_steps.gif">
          <figcaption class="iteration-belt__caption">"a lithograph of a king" / "a lithograph of a volcano"
          </figcaption>
        </figure>


        <figure class="iteration-belt__item">
          <img class="iteration-belt__image" src="media/hybrid/c/hybrid_steps.gif">
          <figcaption class="iteration-belt__caption">"a young woman reading near a candlelight" / "a gothic cathedral"
          </figcaption>
        </figure>

      </div>
    </div>
  </section>


  <script src="../script.js" defer></script>
</body>

</html>